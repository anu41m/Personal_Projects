{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import *\n",
    "from pyspark.sql.functions import *\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "\n",
    "PYSPARK_PYTHON = os.getenv(\"PYSPARK_PYTHON\") \n",
    "PYSPARK_DRIVER_PYTHON = os.getenv(\"PYSPARK_DRIVER_PYTHON\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from delta import configure_spark_with_delta_pip, DeltaTable\n",
    "import json\n",
    "\n",
    "# Load the configuration JSON file\n",
    "with open('/usr/local/spark/conf/spark-defaults.json', 'r') as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "# Initialize the Spark session builder\n",
    "builder = pyspark.sql.SparkSession.builder.appName(\"MyApp1\").config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\").config(\"spark.pyspark.python\", PYSPARK_PYTHON)\\\n",
    "    .config(\"spark.pyspark.driver.python\", PYSPARK_DRIVER_PYTHON)\n",
    "\n",
    "# Read the packages from the text file\n",
    "packages = []\n",
    "with open('/usr/local/spark/conf/packages.txt', 'r') as file:\n",
    "    # Read each line and strip newlines or extra spaces\n",
    "    packages = [line.strip() for line in file if line.strip()]\n",
    "\n",
    "# # Add packages to the Spark session configuration\n",
    "builder.config(\"spark.jars.packages\", \",\".join(packages))\n",
    "\n",
    "# Apply the configurations from the JSON file to the Spark session\n",
    "for key, value in config.items():\n",
    "    builder.config(key, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spark will automatically use the master specified in spark-defaults.conf\n",
    "spark = configure_spark_with_delta_pip(builder).getOrCreate()\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "trgt_path_processed = \"/mnt/Fact/Fact_Parquet/\"\n",
    "trgt_path_csv = \"/mnt/Fact/Fact_Processed/\"\n",
    "source_path = \"/data/Project_Files/Template.xlsm\"\n",
    "sheet_name ='SPENDING_HISTORY'\n",
    "calendar_path =\"/mnt/Calendar/Calendar_Parquet/\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DeltaTable.isDeltaTable(spark, trgt_path_processed):\n",
    "    max_date = spark.sql(f\"select max(date) from delta.`{calendar_path}` WHERE datesk = (select max(datesk) from delta.`{trgt_path_processed}`)\").collect()[0][0] \n",
    "else:\n",
    "    max_date= spark.sql(f\"select min(date) from delta.`{calendar_path}`\").collect()[0][0]\n",
    "print(type(max_date))\n",
    "print(max_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "pandas_df = pd.read_excel(source_path, sheet_name = sheet_name)\n",
    "df=spark.createDataFrame(pandas_df)\n",
    "df= df.withColumn(\"Date\", date_format(df[\"Date\"], \"yyyy-MM-dd\").cast(\"date\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView(\"vw_src\")\n",
    "df_src = spark.sql(f\"select * from vw_src where Date >= '{max_date}'\")\n",
    "df_src.show()\n",
    "print(df_src.count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temp =(\n",
    "df_src\n",
    "    .withColumnRenamed(\"Spending Item\", \"SpendingItem\")\n",
    "    .withColumnRenamed(\"Spend Amount\", \"SpendAmount\")\n",
    "    .withColumnRenamed(\"Wallet used\", \"WalletUsed\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_output =(\n",
    "    df_temp\n",
    "    .withColumn(\"WalletSK\", xxhash64(\"WalletUsed\"))\n",
    "    .withColumn(\"categorysk\", xxhash64(\"category\").cast(\"long\"))\n",
    "    .withColumn(\"DateSK\", regexp_replace(\"date\", \"-\", \"\").cast(\"string\"))\n",
    "    .withColumn(\"PKSK\", xxhash64(concat(\"category\", \"WalletUsed\", \"Index\", \"Date\")).cast(\"string\"))\n",
    "    .withColumn(\"UpdateTimeStamp\", date_format(current_timestamp(), \"yyyy-MM-dd HH:mm:ss\").cast(\"timestamp\"))\n",
    "    .withColumn(\"RowSK\", xxhash64(concat_ws(\"|\", *[col(c) for c in df_temp.columns])))\n",
    "    .drop(\"Index\", \"Date\", \"category\",\"WalletUsed\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_output.createOrReplaceTempView(\"vw_source\")\n",
    "\n",
    "duplicate_counts = spark.sql(\"\"\"\n",
    "    SELECT COUNT(PKSK) as count\n",
    "    FROM vw_source\n",
    "    GROUP BY PKSK\n",
    "    HAVING COUNT(PKSK) > 1\n",
    "\"\"\")\n",
    "x = [row['count'] for row in duplicate_counts.collect()]\n",
    "\n",
    "# Fail the code if there are duplicates\n",
    "if len(x) > 0:\n",
    "    raise ValueError(f\"Duplicate values found :\\n{duplicate_counts}\")\n",
    "else:\n",
    "# # Proceed with the rest of the code if no duplicates\n",
    "    print(\"No duplicates found. Continuing execution...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_name = df_output.columns\n",
    "set_clause = \", \".join([f\"target.{i} = source.{i}\" for i in column_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DeltaTable.isDeltaTable(spark, trgt_path_processed):\n",
    "    query = f\"\"\"MERGE INTO delta.`{trgt_path_processed}` AS target \n",
    "            USING vw_source AS source \n",
    "            ON target.PKSK = source.PKSK \n",
    "            AND target.RowSK <> source.RowSK \n",
    "            WHEN MATCHED THEN UPDATE SET {set_clause}\"\"\"\n",
    "    spark.sql(query)\n",
    "else :\n",
    "    spark.sql(f\"CREATE TABLE delta.`{trgt_path_processed}` USING DELTA AS SELECT * FROM vw_source\")\n",
    "print(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the DataFrame to a CSV file\n",
    "spark.read.format(\"delta\").load(trgt_path_processed).coalesce(1).write.format(\"csv\").option(\"header\",\"true\").mode(\"overwrite\").save(trgt_path_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.read.format(\"csv\").option(\"header\",\"true\").load(trgt_path_csv).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(f\"\"\"SELECT UpdateTimeStamp FROM delta.`{trgt_path_processed}` \n",
    "          WHERE UpdateTimeStamp = (SELECT MAX(UpdateTimeStamp) FROM vw_source)\"\"\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "trgt_copy_path = \"/mnt/Fact/Fact.csv\"\n",
    "files=os.listdir(trgt_path_csv)\n",
    "selected_files = [file for file in files if file.startswith('part-00') and file.endswith('.csv')]\n",
    "file=trgt_path_csv + selected_files[0]\n",
    "print(selected_files)\n",
    "shutil.copy(file, trgt_copy_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

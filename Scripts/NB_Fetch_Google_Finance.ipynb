{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "b657a786",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://460367590f87:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>spark://spark-master:7077</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>MyApp1</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0xffff60547ed0>"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import yfinance as yf\n",
    "import datetime as dt\n",
    "import requests, json, pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from pyspark.sql import *\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import DoubleType\n",
    "import pandas as pd\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "PYSPARK_PYTHON = os.getenv(\"PYSPARK_PYTHON\") \n",
    "PYSPARK_DRIVER_PYTHON = os.getenv(\"PYSPARK_DRIVER_PYTHON\")\n",
    "import pyspark\n",
    "from delta import configure_spark_with_delta_pip, DeltaTable\n",
    "import json\n",
    "\n",
    "# Load the configuration JSON file\n",
    "with open('/usr/local/spark/conf/spark-defaults.json', 'r') as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "# Initialize the Spark session builder\n",
    "builder = pyspark.sql.SparkSession.builder.appName(\"MyApp1\").config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\").config(\"spark.pyspark.python\", PYSPARK_PYTHON)\\\n",
    "    .config(\"spark.pyspark.driver.python\", PYSPARK_DRIVER_PYTHON)\n",
    "\n",
    "# Read the packages from the text file\n",
    "packages = []\n",
    "with open('/usr/local/spark/conf/packages.txt', 'r') as file:\n",
    "    # Read each line and strip newlines or extra spaces\n",
    "    packages = [line.strip() for line in file if line.strip()]\n",
    "\n",
    "# # Add packages to the Spark session configuration\n",
    "builder.config(\"spark.jars.packages\", \",\".join(packages))\n",
    "\n",
    "# Apply the configurations from the JSON file to the Spark session\n",
    "for key, value in config.items():\n",
    "    builder.config(key, value)\n",
    "\n",
    "# Configure Spark with Delta Lake (if needed)\n",
    "spark = configure_spark_with_delta_pip(builder).getOrCreate()\n",
    "# Now you can use the Spark session\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "58e45228",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfpd=pd.read_csv('/home/jovyan/Notebooks/Config_Stock.csv')\n",
    "dfpath=spark.createDataFrame(dfpd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5c43a6e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "trgt_path_processed = dfpath.filter(col(\"DataFeedName\") == \"Stock_Delta_Path\").select('Path').collect()[0][0]\n",
    "trgt_path_csv = dfpath.filter(col(\"DataFeedName\") == \"Stock_CSV_Path\").select('Path').collect()[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cb50d958",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_roe(financials_info, balance_sheet):\n",
    "    net_income = financials_info.loc[\"Net Income\"].iloc[0] if \"Net Income\" in financials_info.index else None\n",
    "    # Fetch Shareholders' Equity from balance sheet\n",
    "    total_equity = balance_sheet.loc[\"Stockholders Equity\"].iloc[0] if \"Stockholders Equity\" in balance_sheet.index else None \n",
    "    return (net_income / total_equity) * 100 if type(net_income) == float and type(total_equity) == float else 0.00"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8bb40760",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_roce(financials_info, balance_sheet):\n",
    "    ebit = financials_info.loc[\"Operating Income\"].iloc[0] if \"Operating Income\" in financials_info.index else 0\n",
    "\n",
    "    # # Get Total Assets and Current Liabilities from balance sheet\n",
    "    total_assets = balance_sheet.loc[\"Total Assets\"].iloc[0] if \"Total Assets\" in balance_sheet.index else 0\n",
    "    current_liabilities = balance_sheet.loc[\"Current Liabilities\"].iloc[0] if \"Current Liabilities\" in balance_sheet.index else 0\n",
    "\n",
    "    # Calculate Capital Employed\n",
    "    capital_employed = total_assets - current_liabilities\n",
    "\n",
    "    return (ebit / capital_employed) * 100 if capital_employed != 0 else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f24f08bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_PEG(stock_info):\n",
    "# Calculate PEG ratios\n",
    "    trailing_pe = stock_info.get(\"trailingPE\", None)\n",
    "    forward_pe = stock_info.get(\"forwardPE\", None)\n",
    "    earnings_growth = stock_info.get(\"earningsGrowth\", None)  # Provided as a decimal\n",
    "\n",
    "    if earnings_growth is not None and earnings_growth > 0:\n",
    "        \n",
    "        trailing_peg = trailing_pe / (earnings_growth * 100) if trailing_pe else 0\n",
    "        forward_peg = forward_pe / (earnings_growth * 100) if forward_pe else 0\n",
    "        peg_t= trailing_peg if trailing_peg else \"N/A\"\n",
    "        peg_f=forward_peg if forward_peg else \"N/A\"\n",
    "    else:\n",
    "        peg_f=peg_t=\"N/A\"\n",
    "    return peg_t,peg_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "11056faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_debt_to_equity(balance_sheet):\n",
    "    total_liabilities = balance_sheet.loc[\"Total Liabilities Net Minority Interest\"].iloc[0] if \"Total Liabilities Net Minority Interest\" in balance_sheet.index else 0\n",
    "    shareholders_equity = balance_sheet.loc[\"Stockholders Equity\"].iloc[0] if \"Stockholders Equity\" in balance_sheet.index else 0\n",
    "    # Calculate Debt-to-Equity Ratio\n",
    "    if shareholders_equity != 0:  # Avoid division by zero\n",
    "        debt_to_equity_ratio = total_liabilities / shareholders_equity\n",
    "    else:\n",
    "        debt_to_equity_ratio = \"N/A\"\n",
    "    return debt_to_equity_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9c438c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_sales_growth(income_statement):\n",
    "    revenue = income_statement.loc[\"Total Revenue\"] if \"Total Revenue\" in income_statement.index else {\"0\":\"NA\"}\n",
    "    revenue = revenue.dropna() if isinstance(revenue, pd.Series) else revenue # Remove any periods with missing data\n",
    "    # Ensure revenue has at least two periods to calculate growth\n",
    "    if len(revenue) > 1:\n",
    "        # Calculate sales growth between the latest two periods\n",
    "        latest_growth = ((revenue.iloc[0] - revenue.iloc[1]) / revenue.iloc[1]) * 100 if revenue.iloc[1] != 0 else 0\n",
    "        latest_period = revenue.index[0].strftime(\"%Y-%m-%d\")\n",
    "    else:\n",
    "        # Handle cases where there isn't enough data\n",
    "        latest_growth=0\n",
    "        latest_period=0\n",
    "    return latest_growth,latest_period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "48b092e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_MA(historical_data):\n",
    "    # Calculate 50-day and 200-day moving averages\n",
    "    if not historical_data.empty:\n",
    "        historical_data[\"MA50\"] = historical_data[\"Close\"].rolling(window=50).mean()\n",
    "        historical_data[\"MA200\"] = historical_data[\"Close\"].rolling(window=200).mean()\n",
    "        # Return the latest MA50 and MA200\n",
    "        latest_data = historical_data.iloc[-1]\n",
    "        ma50=latest_data[\"MA50\"] if latest_data[\"MA50\"] else 0\n",
    "        ma200=latest_data[\"MA200\"] if latest_data[\"MA200\"] else 0\n",
    "    else:\n",
    "        # Return the latest MA50 and MA200\n",
    "        latest_data = 0\n",
    "        ma50=0\n",
    "        ma200=0\n",
    "    return ma50, ma200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "08cfd16d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_stock_data(l_tickers):\n",
    "    ticker_data = []\n",
    "    headers=[\"Ticker\",\"Sector\",\"Industry\",\"52_week_high\",\"ROE\",\"ROCE\",\"Trailing_PEG\",\"Forward_PEG\",\"Debt_to_Equity\",\"Latest_Finanacial_Year\",\"Sales_Growth\",\"MA50\",\"MA200\"]\n",
    "    for t in l_tickers:\n",
    "        ticker = yf.Ticker(t + ('.BO' if t.isdigit() else '.NS'))\n",
    "        stock_info = ticker.info\n",
    "        balance_sheet = ticker.balance_sheet\n",
    "        financials_info=ticker.financials\n",
    "        income_statement=ticker.income_stmt\n",
    "        historical_data = ticker.history(period=\"ytd\")\n",
    "        v_roe=f_roe(financials_info, balance_sheet)\n",
    "        v_roce=f_roce(financials_info, balance_sheet)\n",
    "        v_peg_t,v_peg_f=f_PEG(stock_info)\n",
    "        v_debt_to_equity=f_debt_to_equity(balance_sheet)\n",
    "        v_sales_growth,v_latest_period=f_sales_growth(income_statement)\n",
    "        v_ma50,v_ma200=f_MA(historical_data)\n",
    "        ticker_data.append([t, stock_info.get(\"sector\", \"N/A\"), stock_info.get(\"industry\", \"N/A\"), stock_info.get(\"fiftyTwoWeekHigh\", None),v_roe,v_roce,v_peg_t,v_peg_f,v_debt_to_equity,v_latest_period,v_sales_growth,v_ma50,v_ma200])\n",
    "    df_retun = pd.DataFrame(ticker_data, columns=headers)\n",
    "    return(df_retun)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ee90d349",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# URL to scrape\n",
    "url_link=[\"https://www.google.com/finance/markets/gainers\",\"https://www.google.com/finance/markets/losers\"]\n",
    "rows = []\n",
    "headers=[\"Ticker\",\"Stock_Name\",\"CMP\",\"Change\",\"Change_Percentage\"]\n",
    "\n",
    "for url in url_link:\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        # Find the parent container\n",
    "        parent_container = soup.find('ul', class_='sbnBtf')\n",
    "        if parent_container:\n",
    "            # Find all stock entries within the parent container\n",
    "            stock_entries = parent_container.find_all('li')\n",
    "            for stock in stock_entries:\n",
    "                # Extract relevant details for each stock\n",
    "                stock_ticker = stock.find('div', class_='COaKTb').text if stock.find('div', class_='COaKTb') else \"N/A\"\n",
    "                stock_name = stock.find('div', class_='ZvmM7').text if stock.find('div', class_='ZvmM7') else \"N/A\"\n",
    "                stock_price = stock.find('div', class_='YMlKec').text if stock.find('div', class_='YMlKec') else \"N/A\"\n",
    "                stock_change = stock.find('div', class_='BAftM').text if stock.find('div', class_='BAftM') else \"N/A\"\n",
    "                stock_percent = stock.find('div', class_='zWwE1').text if stock.find('div', class_='zWwE1') else \"N/A\"\n",
    "                # Add extracted data to the list\n",
    "                rows.append([stock_ticker,stock_name,stock_price,stock_change,stock_percent])\n",
    "    \n",
    "    # Convert to JSON string with readable characters\n",
    "df_pd_today = pd.DataFrame(rows, columns=headers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "6965e878",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-01-08 02:57:30.856154\n"
     ]
    }
   ],
   "source": [
    "print(dt.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "bb1b79e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-01-08 10:30:00\n",
      "2025-01-08 03:30:00\n"
     ]
    }
   ],
   "source": [
    "# Get the current date and time\n",
    "current_timestamp = dt.datetime.now()\n",
    "\n",
    "# Replace the time portion with 03:30:00\n",
    "final_timestamp = current_timestamp.replace(hour=3, minute=30, second=0, microsecond=0)\n",
    "\n",
    "print(final_timestamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "e3258bba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi\n"
     ]
    }
   ],
   "source": [
    "if test < final_timestamp:\n",
    "    print(\"helloooo\")\n",
    "else:\n",
    "    print(\"hi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b8e34be3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['EMBDL', 'SPANDANA', 'AEGISLOG', 'GOLDIAM', 'KELLTONTEC', 'PRECWIRE', 'GARUDA', 'ZOTA', 'KIRLOSBROS', 'MAZDA', 'ONEPOINT', 'BTML', 'SINDHUTRAD', 'XTGLOBAL', 'VINEETLAB', 'DICIND', 'MEGASTAR', 'KARMAENG', 'PARAS', 'ORIENTTECH', 'AVONMORE', 'GENESYS', 'UNITEDTEA', 'DONEAR', 'KINGFA', 'TRIGYN', 'VIMTALABS', 'VAKRANGEE', 'MARATHON', 'LLOYDSENT', 'AHLADA', 'BALKRISHNA', 'OSWALGREEN', 'TBZ', 'MVGJL', 'MOBIKWIK', 'COSMOFIRST', 'INDRAMEDCO', 'LLOYDSENGG', 'PTCIL', 'JISLDVREQS', 'RAJRATAN', 'ASMS', 'MGEL', 'INTELLECT', 'JISLJALEQS', 'WORTH', 'UNIDT', 'SDBL', 'RCF', 'SIKKO-RE', 'AVON-RE', 'SPCENET', 'ITI', 'SUVIDHAA', 'SHK', 'THYROCARE', 'NURECA', 'EXCEL', 'KEC', 'GOENKA', 'COMPINFO', 'PNC', 'GANGAFORGE', 'NOIDATOLL', 'EROSMEDIA', 'RCOM', 'MEP', 'LAL', 'JPASSOCIAT', 'A2ZINFRA', 'EDUCOMP', 'SECURCRED', 'MOHITIND', 'EMAMIREAL', 'ALPSINDUS', 'TVVISION', 'ZOMATO', 'TARACHAND', 'SETCO', 'SRM', 'LATTEYS', 'NAUKRI', 'IITL', 'SWIGGY', '360ONE', 'BAIDFIN', 'PALREDTEC', 'DCI', 'PVRINOX', 'SELMC', 'KALYANKJIL', 'SKIL', 'MSPL', 'MUKTAARTS', 'SHAH', 'MAANALU', 'INDSWFTLTD', 'BLUESTARCO', 'ASTRAZEN']\n"
     ]
    }
   ],
   "source": [
    "# Extract unique tickers as a Python list\n",
    "l_tickers = df_pd_today[\"Ticker\"].unique().tolist()\n",
    "# Print the result\n",
    "print(l_tickers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c4ab689a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# t=(\"750940\")\n",
    "# ticker = yf.Ticker(t + \".BO\" if t.isdigit() else t + \".NS\")\n",
    "# stock_info = ticker.info\n",
    "# balance_sheet = ticker.balance_sheet\n",
    "# financials_info=ticker.financials\n",
    "# income_statement=ticker.income_stmt\n",
    "# revenue = income_statement.loc[\"Total Revenue\"] if \"Total Revenue\" in income_statement.index else {\"0\":\"NA\"}\n",
    "# revenue = revenue.dropna() if isinstance(revenue, pd.Series) else revenue # Remove any periods with missing data\n",
    "# # Ensure revenue has at least two periods to calculate growth\n",
    "# if len(revenue) > 1:\n",
    "#     # Calculate sales growth between the latest two periods\n",
    "#     latest_growth = ((revenue.iloc[0] - revenue.iloc[1]) / revenue.iloc[1]) * 100 if revenue.iloc[1] != 0 else 0\n",
    "#     latest_period = revenue.index[0].strftime(\"%Y-%m-%d\")\n",
    "# else:\n",
    "#     # Handle cases where there isn't enough data\n",
    "#     latest_growth=0.0\n",
    "#     latest_period=0.0\n",
    "# print(len(revenue))\n",
    "# print(latest_growth,latest_period)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4f5fb902",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SIKKO-RE.NS: Period 'ytd' is invalid, must be one of ['1d', '5d']\n",
      "$AVON-RE.NS: possibly delisted; no price data found  (period=ytd)\n"
     ]
    }
   ],
   "source": [
    "df_stock_data=f_stock_data(l_tickers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1cd9b46b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_custom=df_pd_today.merge(df_stock_data, on='Ticker',how='left')\n",
    "reorder_colms=[\"Ticker\",\"Stock_Name\",\"Sector\",\"Industry\",\"CMP\",\"Change\",\"Change_Percentage\"]+[col for col in df_custom.columns if col not in [\"Ticker\",\"Stock_Name\",\"Sector\",\"Industry\",\"CMP\",\"Change\",\"Change_Percentage\"]]\n",
    "df_spark=spark.createDataFrame(df_custom[reorder_colms])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fe22719c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_master = df_spark.filter(\n",
    "                            (col(\"ROE\") >= 15) &\n",
    "                            (col(\"ROCE\") >= 15) &\n",
    "                            (col(\"Debt_to_Equity\") <= 1) &\n",
    "                            (col(\"MA50\") >= col(\"MA200\"))\n",
    "                            ).withColumn(\"ROE\",coalesce(round(col(\"ROE\"), 2),lit(0))) \\\n",
    "                            .withColumn(\"ROCE\", coalesce(round(col(\"ROCE\"), 2),lit(0))) \\\n",
    "                            .withColumn(\"Trailing_PEG\", coalesce(round(col(\"Trailing_PEG\"), 2),lit(0))) \\\n",
    "                            .withColumn(\"Forward_PEG\", coalesce(round(col(\"Forward_PEG\"), 2),lit(0))) \\\n",
    "                            .withColumn(\"Debt_to_Equity\", coalesce(round(col(\"Debt_to_Equity\"), 2),lit(0))) \\\n",
    "                            .withColumn(\"Sales_Growth\", coalesce(round(col(\"Sales_Growth\"), 2), lit(0))) \\\n",
    "                            .withColumn(\"MA50\", coalesce(round(col(\"MA50\"), 2), lit(0))) \\\n",
    "                            .withColumn(\"MA200\", coalesce(round(col(\"MA200\"), 2), lit(0))) \\\n",
    "                            .withColumn(\n",
    "                                \"Gainer_Looser\",\n",
    "                                when(\n",
    "                                    regexp_replace(col(\"Change\"), \"â‚¹\", \"\").cast(\"float\") < 0.0, \"L\"\n",
    "                                ).otherwise(\"G\")\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "bfde1fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_read = spark.read.format('delta').load(trgt_path_processed)\n",
    "\n",
    "# Get distinct Tickers\n",
    "# df_test = df_read \\\n",
    "#     .withColumn(\"PKSK\", xxhash64(col(\"Ticker\")).cast(\"string\"))\\\n",
    "#     .withColumn(\"RowSK\", xxhash64(concat_ws(\"|\", *[col(c) for c in df_read.columns])))\\\n",
    "#     .withColumn('UpdateTimestamp', date_format(current_timestamp(), format=\"2025-01-05 10:30:00\").cast('timestamp'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "c70e1148",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_repeat=df_read.join(df_master.select('Ticker'), on='Ticker', how='inner')\\\n",
    "        .withColumn('RunTimeStamp',F.current_timestamp())\\\n",
    "            .withColumn(\"WatchOutFlag\", when(\n",
    "                col('RunTimeStamp') > date_add(col(\"UpdateTimestamp\"), 1), (col('WatchOutFlag') + 1)).otherwise(col('WatchOutFlag'))) \\\n",
    "            .withColumn(\"UpdateTimestamp\",  when(\n",
    "                        col('RunTimeStamp') > date_add(col(\"UpdateTimestamp\"), 1),\n",
    "                        to_timestamp(date_format(current_date(), format=\"yyyy-MM-dd 10:30:00\"))).otherwise(col('UpdateTimestamp')))\\\n",
    "            .drop(\"RunTimeStamp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "7d235d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test=df_master.join(df_repeat, on=\"Ticker\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "127559c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_output=df_master.unionByName(df_repeat, allowMissingColumns=True)\\\n",
    "    .distinct()\\\n",
    "    .withColumn(\"WatchOutFlag\", coalesce(col(\"WatchOutFlag\"), lit(0)).cast('int'))\\\n",
    "    .withColumn('UpdateTimestamp', coalesce(col(\"UpdateTimestamp\"),date_format(F.current_timestamp(), format=\"yyyy-MM-dd 10:30:00\").cast('timestamp')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "a546ecbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "if DeltaTable.isDeltaTable(spark, trgt_path_processed):\n",
    "    df_repeat=df_read.join(df_master.select('Ticker'), on='Ticker', how='inner')\\\n",
    "        .withColumn('RunTimeStamp',F.current_timestamp())\\\n",
    "            .withColumn(\"WatchOutFlag\", when(\n",
    "                col('RunTimeStamp') > date_add(col(\"UpdateTimestamp\"), 1), (col('WatchOutFlag') + 1)).otherwise(col('WatchOutFlag'))) \\\n",
    "            .withColumn(\"UpdateTimestamp\",  when(\n",
    "                        col('RunTimeStamp') > date_add(col(\"UpdateTimestamp\"), 1),\n",
    "                        to_timestamp(date_format(current_date(), format=\"yyyy-MM-dd 10:30:00\"))).otherwise(col('UpdateTimestamp')))\\\n",
    "            .drop(\"RunTimeStamp\")\n",
    "    df_output=df_master.unionByName(df_repeat, allowMissingColumns=True)\\\n",
    "    .distinct()\\\n",
    "    .withColumn(\"WatchOutFlag\", coalesce(col(\"WatchOutFlag\"), lit(0)).cast('int'))\\\n",
    "    .withColumn('UpdateTimestamp', coalesce(col(\"UpdateTimestamp\"),date_format(F.current_timestamp(), format=\"yyyy-MM-dd 10:30:00\").cast('timestamp')))\n",
    "        \n",
    "else:\n",
    "    df_output=df_master.withColumn('WatchOutFlag',lit(0))\\\n",
    "                .withColumn('UpdateTimestamp', date_format(F.current_timestamp(), format=\"yyyy-MM-dd 10:30:00\").cast('timestamp'))\n",
    "df_final=df_output.withColumn(\"PKSK\", xxhash64(col(\"Ticker\")).cast(\"string\"))\\\n",
    "        .withColumn(\"RowSK\", xxhash64(concat_ws(\"|\", *[col(c) for c in df_output.columns])))\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "7adcd938",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.createOrReplaceTempView('vw_source')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "0122d429",
   "metadata": {},
   "outputs": [],
   "source": [
    "query=\"select PKSK from vw_source group by 1 having count(PKSK)>1\"\n",
    "df_dup=spark.sql(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "e5ba153f",
   "metadata": {},
   "outputs": [],
   "source": [
    "query=\"select * from vw_source where PKSK IN (select PKSK from vw_source group by 1 having count(PKSK)>1)\"\n",
    "df_dup=spark.sql(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "e6039500",
   "metadata": {},
   "outputs": [
    {
     "ename": "UnsupportedOperationException",
     "evalue": "[DELTA_MULTIPLE_SOURCE_ROW_MATCHING_TARGET_ROW_IN_MERGE] Cannot perform Merge as multiple source rows matched and attempted to modify the same\ntarget row in the Delta table in possibly conflicting ways. By SQL semantics of Merge,\nwhen multiple source rows match on the same target row, the result may be ambiguous\nas it is unclear which source row should be used to update or delete the matching\ntarget row. You can preprocess the source table to eliminate the possibility of\nmultiple matches. Please refer to\nhttps://docs.delta.io/latest/delta-update.html#upsert-into-a-table-using-merge",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnsupportedOperationException\u001b[0m             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[66], line 12\u001b[0m\n\u001b[1;32m      5\u001b[0m     insert_values\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msource.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m column_name])\n\u001b[1;32m      6\u001b[0m     query \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124mMERGE INTO delta.`\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrgt_path_processed\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` AS target \u001b[39m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;124m            USING vw_source AS source \u001b[39m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;124m            ON target.PKSK = source.PKSK \u001b[39m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;124m            AND target.RowSK <> source.RowSK \u001b[39m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;124m            WHEN MATCHED THEN UPDATE SET \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mset_clause\u001b[38;5;132;01m}\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;124m            WHEN NOT MATCHED THEN INSERT (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00minsert_clause\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) VALUES (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00minsert_values\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m---> 12\u001b[0m     \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m       \n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m :\n\u001b[1;32m     14\u001b[0m     query\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCREATE TABLE delta.`\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrgt_path_processed\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` USING DELTA AS SELECT * FROM vw_source\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/session.py:1631\u001b[0m, in \u001b[0;36mSparkSession.sql\u001b[0;34m(self, sqlQuery, args, **kwargs)\u001b[0m\n\u001b[1;32m   1627\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1628\u001b[0m         litArgs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mPythonUtils\u001b[38;5;241m.\u001b[39mtoArray(\n\u001b[1;32m   1629\u001b[0m             [_to_java_column(lit(v)) \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m (args \u001b[38;5;129;01mor\u001b[39;00m [])]\n\u001b[1;32m   1630\u001b[0m         )\n\u001b[0;32m-> 1631\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[43msqlQuery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlitArgs\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m   1632\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   1633\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(kwargs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mUnsupportedOperationException\u001b[0m: [DELTA_MULTIPLE_SOURCE_ROW_MATCHING_TARGET_ROW_IN_MERGE] Cannot perform Merge as multiple source rows matched and attempted to modify the same\ntarget row in the Delta table in possibly conflicting ways. By SQL semantics of Merge,\nwhen multiple source rows match on the same target row, the result may be ambiguous\nas it is unclear which source row should be used to update or delete the matching\ntarget row. You can preprocess the source table to eliminate the possibility of\nmultiple matches. Please refer to\nhttps://docs.delta.io/latest/delta-update.html#upsert-into-a-table-using-merge"
     ]
    }
   ],
   "source": [
    "if DeltaTable.isDeltaTable(spark, trgt_path_processed):\n",
    "    column_name = df_final.columns\n",
    "    set_clause = \", \".join([f\"target.{i} = source.{i}\" for i in column_name])\n",
    "    insert_clause=\",\".join(column_name)\n",
    "    insert_values=\",\".join([f\"source.{i}\" for i in column_name])\n",
    "    query = f\"\"\"MERGE INTO delta.`{trgt_path_processed}` AS target \n",
    "            USING vw_source AS source \n",
    "            ON target.PKSK = source.PKSK \n",
    "            AND target.RowSK <> source.RowSK \n",
    "            WHEN MATCHED THEN UPDATE SET {set_clause}\n",
    "            WHEN NOT MATCHED THEN INSERT ({insert_clause}) VALUES ({insert_values})\"\"\"\n",
    "    spark.sql(query)       \n",
    "else :\n",
    "    query=f\"CREATE TABLE delta.`{trgt_path_processed}` USING DELTA AS SELECT * FROM vw_source\"\n",
    "    spark.sql(query)\n",
    "\n",
    "print(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d5090ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the DataFrame to a CSV file\n",
    "spark.read.format(\"delta\").load(trgt_path_processed)\\\n",
    "    .coalesce(1).write.format(\"csv\").option(\"header\",\"true\").mode(\"overwrite\").save(trgt_path_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "15f03a61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['part-00000-dbaf40b6-db7f-47d1-b770-100888320bab-c000.csv']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/mnt/Stock_market_data/Processed/processed.csv'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trgt_copy_path = trgt_path_csv + \"processed.csv\"\n",
    "files=os.listdir(trgt_path_csv)\n",
    "selected_files = [file for file in files if file.startswith('part-00') and file.endswith('.csv')]\n",
    "file=trgt_path_csv + selected_files[0]\n",
    "print(selected_files)\n",
    "shutil.copy(file, trgt_copy_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "70ad0d14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "removed /mnt/Stock_market_data/Processed/part-00000-dbaf40b6-db7f-47d1-b770-100888320bab-c000.csv\n",
      "removed /mnt/Stock_market_data/Processed/._SUCCESS.crc\n",
      "removed /mnt/Stock_market_data/Processed/.part-00000-dbaf40b6-db7f-47d1-b770-100888320bab-c000.csv.crc\n",
      "removed /mnt/Stock_market_data/Processed/_SUCCESS\n"
     ]
    }
   ],
   "source": [
    "delete_log = [file for file in files if \"processed.csv\" != file ]\n",
    "for file in delete_log :\n",
    "    os.remove(trgt_path_csv + file)\n",
    "    print(f\"removed {trgt_path_csv + file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0bc61027",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_read=spark.read.format('delta').load(trgt_path_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca3d7f9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

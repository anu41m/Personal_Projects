{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b657a786",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "import datetime as dt\n",
    "import requests, json, pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from pyspark.sql import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import DoubleType\n",
    "import pandas as pd\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "PYSPARK_PYTHON = os.getenv(\"PYSPARK_PYTHON\") \n",
    "PYSPARK_DRIVER_PYTHON = os.getenv(\"PYSPARK_DRIVER_PYTHON\")\n",
    "import pyspark\n",
    "from delta import configure_spark_with_delta_pip, DeltaTable\n",
    "import json\n",
    "\n",
    "# Load the configuration JSON file\n",
    "with open('/usr/local/spark/conf/spark-defaults.json', 'r') as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "# Initialize the Spark session builder\n",
    "builder = pyspark.sql.SparkSession.builder.appName(\"MyApp1\").config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\").config(\"spark.pyspark.python\", PYSPARK_PYTHON)\\\n",
    "    .config(\"spark.pyspark.driver.python\", PYSPARK_DRIVER_PYTHON)\n",
    "\n",
    "# Read the packages from the text file\n",
    "packages = []\n",
    "with open('/usr/local/spark/conf/packages.txt', 'r') as file:\n",
    "    # Read each line and strip newlines or extra spaces\n",
    "    packages = [line.strip() for line in file if line.strip()]\n",
    "\n",
    "# # Add packages to the Spark session configuration\n",
    "builder.config(\"spark.jars.packages\", \",\".join(packages))\n",
    "\n",
    "# Apply the configurations from the JSON file to the Spark session\n",
    "for key, value in config.items():\n",
    "    builder.config(key, value)\n",
    "\n",
    "# Configure Spark with Delta Lake (if needed)\n",
    "spark = configure_spark_with_delta_pip(builder).getOrCreate()\n",
    "# Now you can use the Spark session\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58e45228",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfpath=spark.read.format('csv').load('/home/jovyan/Notebooks/Config_Stock.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c43a6e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "trgt_path_processed = dfpath.filter(col(\"DataFeedName\") == \"Stock_Delta_Path\").select('Path').collect()[0][0]\n",
    "trgt_path_csv = dfpath.filter(col(\"DataFeedName\") == \"Stock_CSV_Path\").select('Path').collect()[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "cb50d958",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_roe(financials_info, balance_sheet):\n",
    "    net_income = financials_info.loc[\"Net Income\"].iloc[0] if \"Net Income\" in financials_info.index else None\n",
    "    # Fetch Shareholders' Equity from balance sheet\n",
    "    total_equity = balance_sheet.loc[\"Stockholders Equity\"].iloc[0] if \"Stockholders Equity\" in balance_sheet.index else None \n",
    "    return (net_income / total_equity) * 100 if type(net_income) == float and type(total_equity) == float else 0.00"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "8bb40760",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_roce(financials_info, balance_sheet):\n",
    "    ebit = financials_info.loc[\"Operating Income\"].iloc[0] if \"Operating Income\" in financials_info.index else 0\n",
    "\n",
    "    # # Get Total Assets and Current Liabilities from balance sheet\n",
    "    total_assets = balance_sheet.loc[\"Total Assets\"].iloc[0] if \"Total Assets\" in balance_sheet.index else 0\n",
    "    current_liabilities = balance_sheet.loc[\"Current Liabilities\"].iloc[0] if \"Current Liabilities\" in balance_sheet.index else 0\n",
    "\n",
    "    # Calculate Capital Employed\n",
    "    capital_employed = total_assets - current_liabilities\n",
    "\n",
    "    return (ebit / capital_employed) * 100 if capital_employed != 0 else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "f24f08bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_PEG(stock_info):\n",
    "# Calculate PEG ratios\n",
    "    trailing_pe = stock_info.get(\"trailingPE\", None)\n",
    "    forward_pe = stock_info.get(\"forwardPE\", None)\n",
    "    earnings_growth = stock_info.get(\"earningsGrowth\", None)  # Provided as a decimal\n",
    "\n",
    "    if earnings_growth is not None and earnings_growth > 0:\n",
    "        \n",
    "        trailing_peg = trailing_pe / (earnings_growth * 100) if trailing_pe else 0\n",
    "        forward_peg = forward_pe / (earnings_growth * 100) if forward_pe else 0\n",
    "        peg_t= trailing_peg if trailing_peg else \"N/A\"\n",
    "        peg_f=forward_peg if forward_peg else \"N/A\"\n",
    "    else:\n",
    "        peg_f=peg_t=\"N/A\"\n",
    "    return peg_t,peg_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "11056faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_debt_to_equity(balance_sheet):\n",
    "    total_liabilities = balance_sheet.loc[\"Total Liabilities Net Minority Interest\"].iloc[0] if \"Total Liabilities Net Minority Interest\" in balance_sheet.index else 0\n",
    "    shareholders_equity = balance_sheet.loc[\"Stockholders Equity\"].iloc[0] if \"Stockholders Equity\" in balance_sheet.index else 0\n",
    "    # Calculate Debt-to-Equity Ratio\n",
    "    if shareholders_equity != 0:  # Avoid division by zero\n",
    "        debt_to_equity_ratio = total_liabilities / shareholders_equity\n",
    "    else:\n",
    "        debt_to_equity_ratio = \"N/A\"\n",
    "    return debt_to_equity_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "9c438c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_sales_growth(income_statement):\n",
    "    revenue = income_statement.loc[\"Total Revenue\"] if \"Total Revenue\" in income_statement.index else {\"0\":\"NA\"}\n",
    "    revenue = revenue.dropna() if isinstance(revenue, pd.Series) else revenue # Remove any periods with missing data\n",
    "    # Ensure revenue has at least two periods to calculate growth\n",
    "    if len(revenue) > 1:\n",
    "        # Calculate sales growth between the latest two periods\n",
    "        latest_growth = ((revenue.iloc[0] - revenue.iloc[1]) / revenue.iloc[1]) * 100 if revenue.iloc[1] != 0 else 0\n",
    "        latest_period = revenue.index[0].strftime(\"%Y-%m-%d\")\n",
    "    else:\n",
    "        # Handle cases where there isn't enough data\n",
    "        latest_growth=0\n",
    "        latest_period=0\n",
    "    return latest_growth,latest_period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "48b092e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_MA(historical_data):\n",
    "    # Calculate 50-day and 200-day moving averages\n",
    "    if not historical_data.empty:\n",
    "        historical_data[\"MA50\"] = historical_data[\"Close\"].rolling(window=50).mean()\n",
    "        historical_data[\"MA200\"] = historical_data[\"Close\"].rolling(window=200).mean()\n",
    "        # Return the latest MA50 and MA200\n",
    "        latest_data = historical_data.iloc[-1]\n",
    "        ma50=latest_data[\"MA50\"] if latest_data[\"MA50\"] else 0\n",
    "        ma200=latest_data[\"MA200\"] if latest_data[\"MA200\"] else 0\n",
    "    else:\n",
    "        # Return the latest MA50 and MA200\n",
    "        latest_data = 0\n",
    "        ma50=0\n",
    "        ma200=0\n",
    "    return ma50, ma200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "08cfd16d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_stock_data(l_tickers):\n",
    "    ticker_data = []\n",
    "    headers=[\"Ticker\",\"Sector\",\"Industry\",\"52-week_high\",\"ROE\",\"ROCE\",\"Trailing_PEG\",\"Forward_PEG\",\"Debt-to-Equity\",\"Latest_Finanacial_Year\",\"Sales_Growth\",\"MA50\",\"MA200\"]\n",
    "    for t in l_tickers:\n",
    "        ticker = yf.Ticker(t + ('.BO' if t.isdigit() else '.NS'))\n",
    "        stock_info = ticker.info\n",
    "        balance_sheet = ticker.balance_sheet\n",
    "        financials_info=ticker.financials\n",
    "        income_statement=ticker.income_stmt\n",
    "        historical_data = ticker.history(period=\"ytd\")\n",
    "        v_roe=f_roe(financials_info, balance_sheet)\n",
    "        v_roce=f_roce(financials_info, balance_sheet)\n",
    "        v_peg_t,v_peg_f=f_PEG(stock_info)\n",
    "        v_debt_to_equity=f_debt_to_equity(balance_sheet)\n",
    "        v_sales_growth,v_latest_period=f_sales_growth(income_statement)\n",
    "        v_ma50,v_ma200=f_MA(historical_data)\n",
    "        ticker_data.append([t, stock_info.get(\"sector\", \"N/A\"), stock_info.get(\"industry\", \"N/A\"), stock_info.get(\"fiftyTwoWeekHigh\", None),v_roe,v_roce,v_peg_t,v_peg_f,v_debt_to_equity,v_latest_period,v_sales_growth,v_ma50,v_ma200])\n",
    "    df_retun = pd.DataFrame(ticker_data, columns=headers)\n",
    "    return(df_retun)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "ee90d349",
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL to scrape\n",
    "url_link=[\"https://www.google.com/finance/markets/gainers\",\"https://www.google.com/finance/markets/losers\"]\n",
    "rows = []\n",
    "headers=[\"Ticker\",\"Stock_Name\",\"CMP\",\"Change\",\"Change_Percentage\"]\n",
    "for url in url_link:\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        # Find the parent container\n",
    "        parent_container = soup.find('ul', class_='sbnBtf')\n",
    "        if parent_container:\n",
    "            # Find all stock entries within the parent container\n",
    "            stock_entries = parent_container.find_all('li')\n",
    "            for stock in stock_entries:\n",
    "                # Extract relevant details for each stock\n",
    "                stock_ticker = stock.find('div', class_='COaKTb').text if stock.find('div', class_='COaKTb') else \"N/A\"\n",
    "                stock_name = stock.find('div', class_='ZvmM7').text if stock.find('div', class_='ZvmM7') else \"N/A\"\n",
    "                stock_price = stock.find('div', class_='YMlKec').text if stock.find('div', class_='YMlKec') else \"N/A\"\n",
    "                stock_change = stock.find('div', class_='BAftM').text if stock.find('div', class_='BAftM') else \"N/A\"\n",
    "                stock_percent = stock.find('div', class_='zWwE1').text if stock.find('div', class_='zWwE1') else \"N/A\"\n",
    "                # Add extracted data to the list\n",
    "                rows.append([stock_ticker,stock_name,stock_price,stock_change,stock_percent])\n",
    "    \n",
    "    # Convert to JSON string with readable characters\n",
    "df_pd_today = pd.DataFrame(rows, columns=headers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e34be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract unique tickers as a Python list\n",
    "l_tickers = df_pd_today[\"Ticker\"].unique().tolist()\n",
    "# Print the result\n",
    "print(l_tickers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "c4ab689a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# t=(\"750940\")\n",
    "# ticker = yf.Ticker(t + \".BO\" if t.isdigit() else t + \".NS\")\n",
    "# stock_info = ticker.info\n",
    "# balance_sheet = ticker.balance_sheet\n",
    "# financials_info=ticker.financials\n",
    "# income_statement=ticker.income_stmt\n",
    "# revenue = income_statement.loc[\"Total Revenue\"] if \"Total Revenue\" in income_statement.index else {\"0\":\"NA\"}\n",
    "# revenue = revenue.dropna() if isinstance(revenue, pd.Series) else revenue # Remove any periods with missing data\n",
    "# # Ensure revenue has at least two periods to calculate growth\n",
    "# if len(revenue) > 1:\n",
    "#     # Calculate sales growth between the latest two periods\n",
    "#     latest_growth = ((revenue.iloc[0] - revenue.iloc[1]) / revenue.iloc[1]) * 100 if revenue.iloc[1] != 0 else 0\n",
    "#     latest_period = revenue.index[0].strftime(\"%Y-%m-%d\")\n",
    "# else:\n",
    "#     # Handle cases where there isn't enough data\n",
    "#     latest_growth=0.0\n",
    "#     latest_period=0.0\n",
    "# print(len(revenue))\n",
    "# print(latest_growth,latest_period)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f5fb902",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stock_data=f_stock_data(l_tickers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "1cd9b46b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_custom=df_pd_today.merge(df_stock_data, on='Ticker',how='left')\n",
    "reorder_colms=[\"Ticker\",\"Stock_Name\",\"Sector\",\"Industry\",\"CMP\",\"Change\",\"Change_Percentage\"]+[col for col in df_custom.columns if col not in [\"Ticker\",\"Stock_Name\",\"Sector\",\"Industry\",\"CMP\",\"Change\",\"Change_Percentage\"]]\n",
    "df_spark=spark.createDataFrame(df_custom[reorder_colms])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "fe22719c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_master = df_spark.filter(\n",
    "                            (col(\"ROE\") >= 15) &\n",
    "                            (col(\"ROCE\") >= 15) &\n",
    "                            (col(\"Debt-to-Equity\") <= 1) &\n",
    "                            (col(\"MA50\") >= col(\"MA200\"))\n",
    "                            ).withColumn(\"ROE\",coalesce(round(col(\"ROE\"), 2),lit(0))) \\\n",
    "                            .withColumn(\"ROCE\", coalesce(round(col(\"ROCE\"), 2),lit(0))) \\\n",
    "                            .withColumn(\"Trailing_PEG\", coalesce(round(col(\"Trailing_PEG\"), 2),lit(0))) \\\n",
    "                            .withColumn(\"Forward_PEG\", coalesce(round(col(\"Forward_PEG\"), 2),lit(0))) \\\n",
    "                            .withColumn(\"Debt-to-Equity\", coalesce(round(col(\"Debt-to-Equity\"), 2),lit(0))) \\\n",
    "                            .withColumn(\"Sales_Growth\", coalesce(round(col(\"Sales_Growth\"), 2), lit(0))) \\\n",
    "                            .withColumn(\"MA50\", coalesce(round(col(\"MA50\"), 2), lit(0))) \\\n",
    "                            .withColumn(\"MA200\", coalesce(round(col(\"MA200\"), 2), lit(0))) \\\n",
    "                            .withColumn(\n",
    "                                \"Gainer/Looser\",\n",
    "                                when(\n",
    "                                    regexp_replace(col(\"Change\"), \"â‚¹\", \"\").cast(\"float\") < 0.0, \"L\"\n",
    "                                ).otherwise(\"G\")\n",
    "                            ).withColumn('WatchOutFlag', lit(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "a546ecbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "if DeltaTable.isDeltaTable(spark, trgt_path_processed):\n",
    "    df_read=spark.read.format('delta').load(trgt_path_processed)\n",
    "    df_repeat=df_master.join(\n",
    "        df_read.select(\"Ticker\",\"UpdateTimestamp\").withColumnRenamed(\"ticker\", \"Ticker\"), on='Ticker', how='inner')\\\n",
    "            .withColumn('RunTimeStamp',current_timestamp())\\\n",
    "            .withColumn(\"WatchOutFlag\", when(\n",
    "                col('RunTimeStamp') > date_add(col(\"UpdateTimestamp\"), 1), (col('WatchOutFlag') + 1)).otherwise(col('WatchOutFlag'))) \\\n",
    "            .withColumn(\"UpdateTimestamp\",  when(\n",
    "                        col('RunTimeStamp') > date_add(col(\"UpdateTimestamp\"), 1),\n",
    "                        to_timestamp(date_format(current_date(), format=\"yyyy-MM-dd 10:30:00\"))).otherwise(col('UpdateTimestamp')))\\\n",
    "            .drop('RunTimeStamp')\n",
    "    df_output=df_master\\\n",
    "        .drop('WatchOutFlag')\\\n",
    "        .join(df_repeat.select('Ticker','WatchOutFlag',\"UpdateTimestamp\"), on='Ticker',how='left') \\\n",
    "        .withColumn('WatchOutFlag', coalesce(col(\"WatchOutFlag\"), lit(0)).cast(\"int\"))\n",
    "        \n",
    "else:\n",
    "    df_output=df_master.withColumn('WatchOutFlag',lit(0))\\\n",
    "                .withColumn('UpdateTimestamp', date_format(current_timestamp(), format=\"yyyy-MM-dd 10:30:00\").cast('timestamp'))\n",
    "df_output.withColumn(\"PKSK\", xxhash64(col(\"Ticker\")).cast(\"string\"))\\\n",
    "        .withColumn(\"RowSK\", xxhash64(concat_ws(\"|\", *[col(c) for c in df_output.columns])))\\\n",
    "        .createOrReplaceTempView('vw_source')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6039500",
   "metadata": {},
   "outputs": [],
   "source": [
    "if DeltaTable.isDeltaTable(spark, trgt_path_processed):\n",
    "    column_name = df_output.columns\n",
    "    set_clause = \", \".join([f\"target.{i} = source.{i}\" for i in column_name])\n",
    "    insert_clause=\",\".join(column_name)\n",
    "    insert_values=\",\".join([f\"source.{i}\" for i in column_name])\n",
    "    query = f\"\"\"MERGE INTO delta.`{trgt_path_processed}` AS target \n",
    "            USING vw_source AS source \n",
    "            ON target.PKSK = source.PKSK \n",
    "            AND target.RowSK <> source.RowSK \n",
    "            WHEN MATCHED THEN UPDATE SET {set_clause}\n",
    "            WHEN NOT MATCHED THEN INSERT ({insert_clause}) VALUES ({insert_values})\"\"\"\n",
    "    spark.sql(query)        \n",
    "else :\n",
    "    query=f\"CREATE TABLE delta.`{trgt_path_processed}` USING DELTA AS SELECT * FROM vw_source\"\n",
    "    spark.sql(query)\n",
    "\n",
    "print(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "0bc61027",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_read=spark.read.format('delta').load(trgt_path_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5090ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the DataFrame to a CSV file\n",
    "spark.read.format(\"delta\").load(trgt_path_processed)\\\n",
    "    .coalesce(1).write.format(\"csv\").option(\"header\",\"true\").mode(\"overwrite\").save(trgt_path_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f03a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "trgt_copy_path = trgt_path_csv + \"processed.csv\"\n",
    "files=os.listdir(trgt_path_csv)\n",
    "selected_files = [file for file in files if file.startswith('part-00') and file.endswith('.csv')]\n",
    "file=trgt_path_csv + selected_files[0]\n",
    "print(selected_files)\n",
    "shutil.copy(file, trgt_copy_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70ad0d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "delete_log = [file for file in files if \"processed.csv\" != file ]\n",
    "for file in delete_log :\n",
    "    os.remove(trgt_path_csv + file)\n",
    "    print(f\"removed {trgt_path_csv + file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca3d7f9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

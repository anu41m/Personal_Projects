{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6cbcf145",
   "metadata": {
    "tags": [
     "papermill-error-cell-tag"
    ]
   },
   "source": [
    "<span style=\"color:red; font-family:Helvetica Neue, Helvetica, Arial, sans-serif; font-size:2em;\">An Exception was encountered at '<a href=\"#papermill-error-cell\">In [6]</a>'.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df902f63",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-06T16:15:29.479557Z",
     "iopub.status.busy": "2025-01-06T16:15:29.477552Z",
     "iopub.status.idle": "2025-01-06T16:15:30.587761Z",
     "shell.execute_reply": "2025-01-06T16:15:30.584922Z"
    },
    "papermill": {
     "duration": 1.142949,
     "end_time": "2025-01-06T16:15:30.594030",
     "exception": false,
     "start_time": "2025-01-06T16:15:29.451081",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import *\n",
    "from pyspark.sql.functions import *\n",
    "import pandas as pd\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "PYSPARK_PYTHON = os.getenv(\"PYSPARK_PYTHON\") \n",
    "PYSPARK_DRIVER_PYTHON = os.getenv(\"PYSPARK_DRIVER_PYTHON\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0bad50bd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-06T16:15:30.645559Z",
     "iopub.status.busy": "2025-01-06T16:15:30.644919Z",
     "iopub.status.idle": "2025-01-06T16:15:30.687955Z",
     "shell.execute_reply": "2025-01-06T16:15:30.686897Z"
    },
    "papermill": {
     "duration": 0.061764,
     "end_time": "2025-01-06T16:15:30.690843",
     "exception": false,
     "start_time": "2025-01-06T16:15:30.629079",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from delta import *\n",
    "import json\n",
    "\n",
    "# Load the configuration JSON file\n",
    "with open('/usr/local/spark/conf/spark-defaults.json', 'r') as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "# Initialize the Spark session builder\n",
    "builder = pyspark.sql.SparkSession.builder.appName(\"MyApp1\").config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\").config(\"spark.pyspark.python\", PYSPARK_PYTHON)\\\n",
    "    .config(\"spark.pyspark.driver.python\", PYSPARK_DRIVER_PYTHON)\n",
    "\n",
    "# Read the packages from the text file\n",
    "packages = []\n",
    "with open('/usr/local/spark/conf/packages.txt', 'r') as file:\n",
    "    # Read each line and strip newlines or extra spaces\n",
    "    packages = [line.strip() for line in file if line.strip()]\n",
    "\n",
    "# # Add packages to the Spark session configuration\n",
    "builder.config(\"spark.jars.packages\", \",\".join(packages))\n",
    "\n",
    "# Apply the configurations from the JSON file to the Spark session\n",
    "for key, value in config.items():\n",
    "    builder.config(key, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "14b38b36",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-06T16:15:30.702336Z",
     "iopub.status.busy": "2025-01-06T16:15:30.701715Z",
     "iopub.status.idle": "2025-01-06T16:15:52.783694Z",
     "shell.execute_reply": "2025-01-06T16:15:52.781359Z"
    },
    "papermill": {
     "duration": 22.120395,
     "end_time": "2025-01-06T16:15:52.816465",
     "exception": false,
     "start_time": "2025-01-06T16:15:30.696070",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://7bd7f7fad00b:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>spark://spark-master:7077</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>MyApp1</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0xffff78327590>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Spark will automatically use the master specified in spark-defaults.conf\n",
    "spark = configure_spark_with_delta_pip(builder).getOrCreate()\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "022a8f0f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-06T16:15:52.864879Z",
     "iopub.status.busy": "2025-01-06T16:15:52.864500Z",
     "iopub.status.idle": "2025-01-06T16:15:56.248615Z",
     "shell.execute_reply": "2025-01-06T16:15:56.241482Z"
    },
    "papermill": {
     "duration": 3.46731,
     "end_time": "2025-01-06T16:15:56.296887",
     "exception": false,
     "start_time": "2025-01-06T16:15:52.829577",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "dfconfig=pd.read_csv('/home/jovyan/Notebooks/configpath.csv')\n",
    "dfpath=spark.createDataFrame(dfconfig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "07da023f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-06T16:15:56.381420Z",
     "iopub.status.busy": "2025-01-06T16:15:56.355291Z",
     "iopub.status.idle": "2025-01-06T16:16:10.632722Z",
     "shell.execute_reply": "2025-01-06T16:16:10.627458Z"
    },
    "papermill": {
     "duration": 14.336003,
     "end_time": "2025-01-06T16:16:10.648322",
     "exception": false,
     "start_time": "2025-01-06T16:15:56.312319",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "trgt_path_processed = dfpath.filter(col(\"DataFeedName\") == \"Category_Delta_Path\").select('Path').collect()[0][0]\n",
    "trgt_path_csv = dfpath.filter(col(\"DataFeedName\") == \"Category_CSV_Path\").select('Path').collect()[0][0]\n",
    "source_path = dfpath.filter(col(\"DataFeedName\") == \"FactFile\").select('Path').collect()[0][0]\n",
    "v_sheet_name =dfpath.filter(col(\"DataFeedName\") == \"sheet_name\").select('Path').collect()[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0997cb36",
   "metadata": {
    "tags": [
     "papermill-error-cell-tag"
    ]
   },
   "source": [
    "<span id=\"papermill-error-cell\" style=\"color:red; font-family:Helvetica Neue, Helvetica, Arial, sans-serif; font-size:2em;\">Execution using papermill encountered an exception here and stopped:</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "46e58ddf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-06T16:16:10.674454Z",
     "iopub.status.busy": "2025-01-06T16:16:10.674171Z",
     "iopub.status.idle": "2025-01-06T16:16:11.383308Z",
     "shell.execute_reply": "2025-01-06T16:16:11.382150Z"
    },
    "papermill": {
     "duration": 0.723334,
     "end_time": "2025-01-06T16:16:11.388142",
     "exception": true,
     "start_time": "2025-01-06T16:16:10.664808",
     "status": "failed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[Errno 35] Resource deadlock avoided",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Read the Excel file (use Spark-Excel library)\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_excel\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msheet_name\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mv_sheet_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m df \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39mcreateDataFrame(df)\n\u001b[1;32m      4\u001b[0m df\u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mwithColumn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDate\u001b[39m\u001b[38;5;124m\"\u001b[39m, to_date(df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDate\u001b[39m\u001b[38;5;124m\"\u001b[39m],\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myyyy-MM-dd\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/io/excel/_base.py:478\u001b[0m, in \u001b[0;36mread_excel\u001b[0;34m(io, sheet_name, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, parse_dates, date_parser, date_format, thousands, decimal, comment, skipfooter, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    476\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(io, ExcelFile):\n\u001b[1;32m    477\u001b[0m     should_close \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 478\u001b[0m     io \u001b[38;5;241m=\u001b[39m \u001b[43mExcelFile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mio\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mengine\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    479\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m engine \u001b[38;5;129;01mand\u001b[39;00m engine \u001b[38;5;241m!=\u001b[39m io\u001b[38;5;241m.\u001b[39mengine:\n\u001b[1;32m    480\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    481\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEngine should not be specified when passing \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    482\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124man ExcelFile - ExcelFile already has the engine set\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    483\u001b[0m     )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/io/excel/_base.py:1496\u001b[0m, in \u001b[0;36mExcelFile.__init__\u001b[0;34m(self, path_or_buffer, engine, storage_options)\u001b[0m\n\u001b[1;32m   1494\u001b[0m     ext \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxls\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1495\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1496\u001b[0m     ext \u001b[38;5;241m=\u001b[39m \u001b[43minspect_excel_format\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1497\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcontent_or_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\n\u001b[1;32m   1498\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1499\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ext \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1501\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExcel file format cannot be determined, you must specify \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1502\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124man engine manually.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1503\u001b[0m         )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/io/excel/_base.py:1376\u001b[0m, in \u001b[0;36minspect_excel_format\u001b[0;34m(content_or_path, storage_options)\u001b[0m\n\u001b[1;32m   1374\u001b[0m stream \u001b[38;5;241m=\u001b[39m handle\u001b[38;5;241m.\u001b[39mhandle\n\u001b[1;32m   1375\u001b[0m stream\u001b[38;5;241m.\u001b[39mseek(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m-> 1376\u001b[0m buf \u001b[38;5;241m=\u001b[39m \u001b[43mstream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mPEEK_SIZE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1377\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m buf \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1378\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream is empty\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mOSError\u001b[0m: [Errno 35] Resource deadlock avoided"
     ]
    }
   ],
   "source": [
    "# Read the Excel file (use Spark-Excel library)\n",
    "df = pd.read_excel(source_path, sheet_name = v_sheet_name)\n",
    "df = spark.createDataFrame(df)\n",
    "df= df.withColumn(\"Date\", to_date(df[\"Date\"],\"yyyy-MM-dd\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "114e1ad2",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView(\"vw_src\")\n",
    "query = \"select distinct Category from vw_src\"\n",
    "print(query)\n",
    "df_src = spark.sql(query)\n",
    "df_src.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e4bf5c",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_output = \\\n",
    "df_src.withColumn(\"categorysk\",xxhash64(\"category\"))\\\n",
    "        .withColumn(\"UpdateTimeStamp\", date_format(current_timestamp(), \"yyyy-MM-dd HH:mm:ss\").cast(\"timestamp\"))\\\n",
    "            .withColumn(\"RowSK\",xxhash64(concat_ws(\"|\", *[col(c) for c in df_src.columns])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd9f8fe",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_output.createOrReplaceTempView(\"vw_source\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e51f1490",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_output.createOrReplaceTempView(\"vw_source\")\n",
    "if DeltaTable.isDeltaTable(spark, trgt_path_processed):\n",
    "    column_name = df_output.columns\n",
    "    set_clause = \", \".join([f\"target.{i} = source.{i}\" for i in column_name])\n",
    "    insert_clause=\",\".join(column_name)\n",
    "    insert_values=\",\".join([f\"source.{i}\" for i in column_name])\n",
    "    query = f\"\"\"MERGE INTO delta.`{trgt_path_processed}` AS target \n",
    "            USING vw_source AS source \n",
    "            ON target.categorysk = source.categorysk \n",
    "            AND target.RowSK <> source.RowSK \n",
    "            WHEN MATCHED THEN UPDATE SET {set_clause}\n",
    "            WHEN NOT MATCHED THEN INSERT ({insert_clause}) VALUES ({insert_values})\"\"\"\n",
    "else :\n",
    "    query=f\"CREATE TABLE delta.`{trgt_path_processed}` USING DELTA AS SELECT * FROM vw_source\"\n",
    "print(query)\n",
    "spark.sql(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ed0a24",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Save the DataFrame to a CSV file\n",
    "spark.read.format(\"delta\").load(trgt_path_processed).coalesce(1).write.format(\"csv\").option(\"header\",\"true\").mode(\"overwrite\").save(trgt_path_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc79a952",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "trgt_copy_path = trgt_path_csv + \"processed.csv\"\n",
    "files=os.listdir(trgt_path_csv)\n",
    "selected_files = [file for file in files if file.startswith('part-00') and file.endswith('.csv')]\n",
    "file=trgt_path_csv + selected_files[0]\n",
    "print(selected_files)\n",
    "shutil.copy(file, trgt_copy_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0265523f",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "delete_log = [file for file in files if \"processed.csv\" != file ]\n",
    "for file in delete_log :\n",
    "    os.remove(trgt_path_csv + file)\n",
    "    print(f\"removed {trgt_path_csv + file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bc7260c",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 48.368161,
   "end_time": "2025-01-06T16:16:14.079706",
   "environment_variables": {},
   "exception": true,
   "input_path": "/home/jovyan/Notebooks/NB_DIM_Category.ipynb",
   "output_path": "/home/jovyan/Notebooks/output/NB_DIM_Category.ipynb",
   "parameters": {},
   "start_time": "2025-01-06T16:15:25.711545",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
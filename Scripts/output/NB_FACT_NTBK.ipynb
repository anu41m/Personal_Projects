{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9fead2bf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-06T16:15:29.799208Z",
     "iopub.status.busy": "2025-01-06T16:15:29.798902Z",
     "iopub.status.idle": "2025-01-06T16:15:30.639059Z",
     "shell.execute_reply": "2025-01-06T16:15:30.636381Z"
    },
    "papermill": {
     "duration": 0.905544,
     "end_time": "2025-01-06T16:15:30.645532",
     "exception": false,
     "start_time": "2025-01-06T16:15:29.739988",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import *\n",
    "from pyspark.sql.functions import *\n",
    "import pandas as pd\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "PYSPARK_PYTHON = os.getenv(\"PYSPARK_PYTHON\") \n",
    "PYSPARK_DRIVER_PYTHON = os.getenv(\"PYSPARK_DRIVER_PYTHON\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b9166a84",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-06T16:15:30.673583Z",
     "iopub.status.busy": "2025-01-06T16:15:30.673174Z",
     "iopub.status.idle": "2025-01-06T16:15:30.689913Z",
     "shell.execute_reply": "2025-01-06T16:15:30.687673Z"
    },
    "papermill": {
     "duration": 0.036193,
     "end_time": "2025-01-06T16:15:30.692196",
     "exception": false,
     "start_time": "2025-01-06T16:15:30.656003",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from delta import configure_spark_with_delta_pip, DeltaTable\n",
    "import json\n",
    "\n",
    "# Load the configuration JSON file\n",
    "with open('/usr/local/spark/conf/spark-defaults.json', 'r') as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "# Initialize the Spark session builder\n",
    "builder = pyspark.sql.SparkSession.builder.appName(\"MyApp1\").config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\").config(\"spark.pyspark.python\", PYSPARK_PYTHON)\\\n",
    "    .config(\"spark.pyspark.driver.python\", PYSPARK_DRIVER_PYTHON)\n",
    "\n",
    "# Read the packages from the text file\n",
    "packages = []\n",
    "with open('/usr/local/spark/conf/packages.txt', 'r') as file:\n",
    "    # Read each line and strip newlines or extra spaces\n",
    "    packages = [line.strip() for line in file if line.strip()]\n",
    "\n",
    "# # Add packages to the Spark session configuration\n",
    "builder.config(\"spark.jars.packages\", \",\".join(packages))\n",
    "\n",
    "# Apply the configurations from the JSON file to the Spark session\n",
    "for key, value in config.items():\n",
    "    builder.config(key, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f7a7bf8e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-06T16:15:30.704924Z",
     "iopub.status.busy": "2025-01-06T16:15:30.704439Z",
     "iopub.status.idle": "2025-01-06T16:16:04.171245Z",
     "shell.execute_reply": "2025-01-06T16:16:04.161690Z"
    },
    "papermill": {
     "duration": 33.533087,
     "end_time": "2025-01-06T16:16:04.230316",
     "exception": false,
     "start_time": "2025-01-06T16:15:30.697229",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://6caeacb52bee:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>spark://spark-master:7077</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>MyApp1</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0xffff5f928d10>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Spark will automatically use the master specified in spark-defaults.conf\n",
    "spark = configure_spark_with_delta_pip(builder).getOrCreate()\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1e73aa7f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-06T16:16:04.261535Z",
     "iopub.status.busy": "2025-01-06T16:16:04.258894Z",
     "iopub.status.idle": "2025-01-06T16:16:06.133272Z",
     "shell.execute_reply": "2025-01-06T16:16:06.126895Z"
    },
    "papermill": {
     "duration": 1.915361,
     "end_time": "2025-01-06T16:16:06.158404",
     "exception": false,
     "start_time": "2025-01-06T16:16:04.243043",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "dfconfig=pd.read_csv('/home/jovyan/Notebooks/configpath.csv')\n",
    "dfpath=spark.createDataFrame(dfconfig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86763cf6",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": false,
     "start_time": "2025-01-06T16:16:06.187361",
     "status": "running"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "trgt_path_processed = dfpath.filter(col(\"DataFeedName\") == \"Fact_Delta_Path\").select('Path').collect()[0][0]\n",
    "trgt_path_csv = dfpath.filter(col(\"DataFeedName\") == \"Fact_CSV_Path\").select('Path').collect()[0][0]\n",
    "source_path = dfpath.filter(col(\"DataFeedName\") == \"FactFile\").select('Path').collect()[0][0]\n",
    "sheet_name =dfpath.filter(col(\"DataFeedName\") == \"sheet_name\").select('Path').collect()[0][0]\n",
    "calendar_path =dfpath.filter(col(\"DataFeedName\") == \"Calendar_Delta_Path\").select('Path').collect()[0][0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70dc7019",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "pandas_df = pd.read_excel(source_path, sheet_name = sheet_name)\n",
    "df=spark.createDataFrame(pandas_df)\n",
    "df= df.withColumn(\"Date\", date_format(df[\"Date\"], \"yyyy-MM-dd\").cast(\"date\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d7a56b4",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if DeltaTable.isDeltaTable(spark, trgt_path_processed):\n",
    "    dffact = spark.sql(f\"SELECT MAX(datesk) as max_date FROM delta.`{trgt_path_processed}`\")\n",
    "    # Collect the result\n",
    "    max_date = dffact.withColumn(\"max_date\",\n",
    "                                    concat_ws(\"-\",col(\"max_date\").substr(1,4),col(\"max_date\").substr(5,2),col(\"max_date\").substr(7,2))\n",
    "                                    .cast(\"date\")).collect()[0][\"max_date\"]\n",
    "    query=f\"SELECT * FROM vw_src WHERE Date >= '{max_date}'\"\n",
    "    \n",
    "else:\n",
    "    query=\"SELECT * FROM vw_src\"\n",
    "print(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f54b018",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView(\"vw_src\")\n",
    "df_src = spark.sql(query)\n",
    "df_src.show()\n",
    "print(df_src.count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d6ae1fc",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_temp =(\n",
    "df_src\n",
    "    .withColumnRenamed(\"Spending Item\", \"SpendingItem\")\n",
    "    .withColumnRenamed(\"Spend Amount\", \"SpendAmount\")\n",
    "    .withColumnRenamed(\"Wallet used\", \"WalletUsed\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d419ef37",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "df_output =(\n",
    "    df_temp\n",
    "    .withColumn(\"WalletSK\", xxhash64(\"WalletUsed\"))\n",
    "    .withColumn(\"categorysk\", xxhash64(\"category\").cast(\"long\"))\n",
    "    .withColumn(\"DateSK\", regexp_replace(\"date\", \"-\", \"\").cast(\"string\"))\n",
    "    .withColumn(\"PKSK\", xxhash64(concat(\"category\", \"WalletUsed\", \"Index\", \"Date\")).cast(\"string\"))\n",
    "    .withColumn(\"UpdateTimeStamp\", date_format(current_timestamp(), \"yyyy-MM-dd HH:mm:ss\").cast(\"timestamp\"))\n",
    "    .withColumn(\"RowSK\", xxhash64(concat_ws(\"|\", *[col(c) for c in df_temp.columns])))\n",
    "    .drop(\"Index\", \"Date\", \"category\",\"WalletUsed\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bd33ea2",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_output.createOrReplaceTempView(\"vw_source\")\n",
    "\n",
    "duplicate_counts = spark.sql(\"\"\"\n",
    "    SELECT COUNT(PKSK) as count\n",
    "    FROM vw_source\n",
    "    GROUP BY PKSK\n",
    "    HAVING COUNT(PKSK) > 1\n",
    "\"\"\")\n",
    "x = [row['count'] for row in duplicate_counts.collect()]\n",
    "\n",
    "# Fail the code if there are duplicates\n",
    "if len(x) > 0:\n",
    "    raise ValueError(f\"Duplicate values found :\\n{duplicate_counts}\")\n",
    "else:\n",
    "# # Proceed with the rest of the code if no duplicates\n",
    "    print(\"No duplicates found. Continuing execution...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4df1db96",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if DeltaTable.isDeltaTable(spark, trgt_path_processed):\n",
    "    column_name = df_output.columns\n",
    "    set_clause = \", \".join([f\"target.{i} = source.{i}\" for i in column_name])\n",
    "    insert_clause=\",\".join(column_name)\n",
    "    insert_values=\",\".join([f\"source.{i}\" for i in column_name])\n",
    "    query = f\"\"\"MERGE INTO delta.`{trgt_path_processed}` AS target \n",
    "            USING vw_source AS source \n",
    "            ON target.PKSK = source.PKSK \n",
    "            AND target.RowSK <> source.RowSK \n",
    "            WHEN MATCHED THEN UPDATE SET {set_clause}\n",
    "            WHEN NOT MATCHED THEN INSERT ({insert_clause}) VALUES ({insert_values})\"\"\"\n",
    "    spark.sql(query)        \n",
    "else :\n",
    "    query=f\"CREATE TABLE delta.`{trgt_path_processed}` USING DELTA AS SELECT * FROM vw_source\"\n",
    "    spark.sql(query)\n",
    "\n",
    "print(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "391f86db",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Save the DataFrame to a CSV file\n",
    "spark.read.format(\"delta\").load(trgt_path_processed).coalesce(1).write.format(\"csv\").option(\"header\",\"true\").mode(\"overwrite\").save(trgt_path_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a81da33b",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "spark.read.format(\"csv\").option(\"header\",\"true\").load(trgt_path_csv).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd5d173",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "spark.sql(f\"\"\"SELECT UpdateTimeStamp FROM delta.`{trgt_path_processed}` \n",
    "          WHERE UpdateTimeStamp = (SELECT MAX(UpdateTimeStamp) FROM vw_source)\"\"\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21bdd3f4",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "trgt_copy_path = trgt_path_csv + \"processed.csv\"\n",
    "files=os.listdir(trgt_path_csv)\n",
    "selected_files = [file for file in files if file.startswith('part-00') and file.endswith('.csv')]\n",
    "file=trgt_path_csv + selected_files[0]\n",
    "print(selected_files)\n",
    "shutil.copy(file, trgt_copy_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eba44174",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "delete_log = [file for file in files if \"processed.csv\" != file ]\n",
    "for file in delete_log :\n",
    "    os.remove(trgt_path_csv + file)\n",
    "    print(f\"removed {trgt_path_csv + file}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "papermill": {
   "default_parameters": {},
   "duration": null,
   "end_time": null,
   "environment_variables": {},
   "exception": null,
   "input_path": "/home/jovyan/Notebooks/NB_FACT_NTBK.ipynb",
   "output_path": "/home/jovyan/Notebooks/output/NB_FACT_NTBK.ipynb",
   "parameters": {},
   "start_time": "2025-01-06T16:15:25.776155",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
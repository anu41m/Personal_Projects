{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d4e7f77",
   "metadata": {
    "tags": [
     "papermill-error-cell-tag"
    ]
   },
   "source": [
    "<span style=\"color:red; font-family:Helvetica Neue, Helvetica, Arial, sans-serif; font-size:2em;\">An Exception was encountered at '<a href=\"#papermill-error-cell\">In [12]</a>'.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "26b21222",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-28T11:03:49.577490Z",
     "iopub.status.busy": "2024-12-28T11:03:49.576331Z",
     "iopub.status.idle": "2024-12-28T11:03:51.279626Z",
     "shell.execute_reply": "2024-12-28T11:03:51.276088Z"
    },
    "papermill": {
     "duration": 1.755832,
     "end_time": "2024-12-28T11:03:51.296724",
     "exception": false,
     "start_time": "2024-12-28T11:03:49.540892",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import *\n",
    "from pyspark.sql.functions import *\n",
    "import pandas as pd\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "PYSPARK_PYTHON = os.getenv(\"PYSPARK_PYTHON\") \n",
    "PYSPARK_DRIVER_PYTHON = os.getenv(\"PYSPARK_DRIVER_PYTHON\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8f66491a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-28T11:03:51.333315Z",
     "iopub.status.busy": "2024-12-28T11:03:51.332395Z",
     "iopub.status.idle": "2024-12-28T11:03:51.360780Z",
     "shell.execute_reply": "2024-12-28T11:03:51.355597Z"
    },
    "papermill": {
     "duration": 0.041172,
     "end_time": "2024-12-28T11:03:51.363910",
     "exception": false,
     "start_time": "2024-12-28T11:03:51.322738",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from delta import configure_spark_with_delta_pip, DeltaTable\n",
    "import json\n",
    "\n",
    "# Load the configuration JSON file\n",
    "with open('/usr/local/spark/conf/spark-defaults.json', 'r') as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "# Initialize the Spark session builder\n",
    "builder = pyspark.sql.SparkSession.builder.appName(\"MyApp1\").config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\").config(\"spark.pyspark.python\", PYSPARK_PYTHON)\\\n",
    "    .config(\"spark.pyspark.driver.python\", PYSPARK_DRIVER_PYTHON)\n",
    "\n",
    "# Read the packages from the text file\n",
    "packages = []\n",
    "with open('/usr/local/spark/conf/packages.txt', 'r') as file:\n",
    "    # Read each line and strip newlines or extra spaces\n",
    "    packages = [line.strip() for line in file if line.strip()]\n",
    "\n",
    "# # Add packages to the Spark session configuration\n",
    "builder.config(\"spark.jars.packages\", \",\".join(packages))\n",
    "\n",
    "# Apply the configurations from the JSON file to the Spark session\n",
    "for key, value in config.items():\n",
    "    builder.config(key, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d06de33d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-28T11:03:51.397596Z",
     "iopub.status.busy": "2024-12-28T11:03:51.397053Z",
     "iopub.status.idle": "2024-12-28T11:04:27.135721Z",
     "shell.execute_reply": "2024-12-28T11:04:27.132404Z"
    },
    "papermill": {
     "duration": 35.822153,
     "end_time": "2024-12-28T11:04:27.201993",
     "exception": false,
     "start_time": "2024-12-28T11:03:51.379840",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://ee28d3795bb3:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>spark://spark-master:7077</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>MyApp1</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0xffff6641fe90>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Spark will automatically use the master specified in spark-defaults.conf\n",
    "spark = configure_spark_with_delta_pip(builder).getOrCreate()\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8b2ac706",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-28T11:04:27.233722Z",
     "iopub.status.busy": "2024-12-28T11:04:27.233434Z",
     "iopub.status.idle": "2024-12-28T11:04:29.636774Z",
     "shell.execute_reply": "2024-12-28T11:04:29.634349Z"
    },
    "papermill": {
     "duration": 2.438811,
     "end_time": "2024-12-28T11:04:29.649590",
     "exception": false,
     "start_time": "2024-12-28T11:04:27.210779",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "dfconfig=pd.read_csv('/home/jovyan/Notebooks/configpath.csv')\n",
    "dfpath=spark.createDataFrame(dfconfig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0e27252f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-28T11:04:29.682102Z",
     "iopub.status.busy": "2024-12-28T11:04:29.681812Z",
     "iopub.status.idle": "2024-12-28T11:04:37.319886Z",
     "shell.execute_reply": "2024-12-28T11:04:37.318319Z"
    },
    "papermill": {
     "duration": 7.660752,
     "end_time": "2024-12-28T11:04:37.329684",
     "exception": false,
     "start_time": "2024-12-28T11:04:29.668932",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "trgt_path_processed = dfpath.filter(col(\"DataFeedName\") == \"Fact_Delta_Path\").select('Path').collect()[0][0]\n",
    "trgt_path_csv = dfpath.filter(col(\"DataFeedName\") == \"Fact_CSV_Path\").select('Path').collect()[0][0]\n",
    "source_path = dfpath.filter(col(\"DataFeedName\") == \"FactFile\").select('Path').collect()[0][0]\n",
    "sheet_name =dfpath.filter(col(\"DataFeedName\") == \"sheet_name\").select('Path').collect()[0][0]\n",
    "calendar_path =dfpath.filter(col(\"DataFeedName\") == \"Calendar_Delta_Path\").select('Path').collect()[0][0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fbb423c9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-28T11:04:37.348877Z",
     "iopub.status.busy": "2024-12-28T11:04:37.348664Z",
     "iopub.status.idle": "2024-12-28T11:04:38.307313Z",
     "shell.execute_reply": "2024-12-28T11:04:38.306365Z"
    },
    "papermill": {
     "duration": 0.983206,
     "end_time": "2024-12-28T11:04:38.325562",
     "exception": false,
     "start_time": "2024-12-28T11:04:37.342356",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "pandas_df = pd.read_excel(source_path, sheet_name = sheet_name)\n",
    "df=spark.createDataFrame(pandas_df)\n",
    "df= df.withColumn(\"Date\", date_format(df[\"Date\"], \"yyyy-MM-dd\").cast(\"date\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "58296d05",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-28T11:04:38.342855Z",
     "iopub.status.busy": "2024-12-28T11:04:38.342569Z",
     "iopub.status.idle": "2024-12-28T11:05:02.070167Z",
     "shell.execute_reply": "2024-12-28T11:05:02.068206Z"
    },
    "papermill": {
     "duration": 23.761535,
     "end_time": "2024-12-28T11:05:02.098671",
     "exception": false,
     "start_time": "2024-12-28T11:04:38.337136",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELECT * FROM vw_src WHERE Date >= '2024-12-20'\n"
     ]
    }
   ],
   "source": [
    "if DeltaTable.isDeltaTable(spark, trgt_path_processed):\n",
    "    dffact = spark.sql(f\"SELECT MAX(datesk) as max_date FROM delta.`{trgt_path_processed}`\")\n",
    "    # Collect the result\n",
    "    max_date = dffact.withColumn(\"max_date\",\n",
    "                                    concat_ws(\"-\",col(\"max_date\").substr(1,4),col(\"max_date\").substr(5,2),col(\"max_date\").substr(7,2))\n",
    "                                    .cast(\"date\")).collect()[0][\"max_date\"]\n",
    "    query=f\"SELECT * FROM vw_src WHERE Date >= '{max_date}'\"\n",
    "    \n",
    "else:\n",
    "    query=\"SELECT * FROM vw_src\"\n",
    "print(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e7221fe7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-28T11:05:02.119218Z",
     "iopub.status.busy": "2024-12-28T11:05:02.118991Z",
     "iopub.status.idle": "2024-12-28T11:05:07.158963Z",
     "shell.execute_reply": "2024-12-28T11:05:07.153431Z"
    },
    "papermill": {
     "duration": 5.06279,
     "end_time": "2024-12-28T11:05:07.176397",
     "exception": false,
     "start_time": "2024-12-28T11:05:02.113607",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+-------------+-----------+--------+------------+\n",
      "|Index|      Date|Spending Item|Wallet used|Category|Spend Amount|\n",
      "+-----+----------+-------------+-----------+--------+------------+\n",
      "|   55|2024-12-20|       Swiggy|  ICICI Pay|Personal|         199|\n",
      "+-----+----------+-------------+-----------+--------+------------+\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "df.createOrReplaceTempView(\"vw_src\")\n",
    "df_src = spark.sql(query)\n",
    "df_src.show()\n",
    "print(df_src.count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "487d170a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-28T11:05:07.196244Z",
     "iopub.status.busy": "2024-12-28T11:05:07.195855Z",
     "iopub.status.idle": "2024-12-28T11:05:07.262242Z",
     "shell.execute_reply": "2024-12-28T11:05:07.261401Z"
    },
    "papermill": {
     "duration": 0.080303,
     "end_time": "2024-12-28T11:05:07.265648",
     "exception": false,
     "start_time": "2024-12-28T11:05:07.185345",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_temp =(\n",
    "df_src\n",
    "    .withColumnRenamed(\"Spending Item\", \"SpendingItem\")\n",
    "    .withColumnRenamed(\"Spend Amount\", \"SpendAmount\")\n",
    "    .withColumnRenamed(\"Wallet used\", \"WalletUsed\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e2abc55a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-28T11:05:07.285147Z",
     "iopub.status.busy": "2024-12-28T11:05:07.284308Z",
     "iopub.status.idle": "2024-12-28T11:05:08.184803Z",
     "shell.execute_reply": "2024-12-28T11:05:08.182450Z"
    },
    "papermill": {
     "duration": 0.937671,
     "end_time": "2024-12-28T11:05:08.212055",
     "exception": false,
     "start_time": "2024-12-28T11:05:07.274384",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "df_output =(\n",
    "    df_temp\n",
    "    .withColumn(\"WalletSK\", xxhash64(\"WalletUsed\"))\n",
    "    .withColumn(\"categorysk\", xxhash64(\"category\").cast(\"long\"))\n",
    "    .withColumn(\"DateSK\", regexp_replace(\"date\", \"-\", \"\").cast(\"string\"))\n",
    "    .withColumn(\"PKSK\", xxhash64(concat(\"category\", \"WalletUsed\", \"Index\", \"Date\")).cast(\"string\"))\n",
    "    .withColumn(\"UpdateTimeStamp\", date_format(current_timestamp(), \"yyyy-MM-dd HH:mm:ss\").cast(\"timestamp\"))\n",
    "    .withColumn(\"RowSK\", xxhash64(concat_ws(\"|\", *[col(c) for c in df_temp.columns])))\n",
    "    .drop(\"Index\", \"Date\", \"category\",\"WalletUsed\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bcb05a0d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-28T11:05:08.276438Z",
     "iopub.status.busy": "2024-12-28T11:05:08.275750Z",
     "iopub.status.idle": "2024-12-28T11:05:12.278140Z",
     "shell.execute_reply": "2024-12-28T11:05:12.269381Z"
    },
    "papermill": {
     "duration": 4.052751,
     "end_time": "2024-12-28T11:05:12.291910",
     "exception": false,
     "start_time": "2024-12-28T11:05:08.239159",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No duplicates found. Continuing execution...\n"
     ]
    }
   ],
   "source": [
    "df_output.createOrReplaceTempView(\"vw_source\")\n",
    "\n",
    "duplicate_counts = spark.sql(\"\"\"\n",
    "    SELECT COUNT(PKSK) as count\n",
    "    FROM vw_source\n",
    "    GROUP BY PKSK\n",
    "    HAVING COUNT(PKSK) > 1\n",
    "\"\"\")\n",
    "x = [row['count'] for row in duplicate_counts.collect()]\n",
    "\n",
    "# Fail the code if there are duplicates\n",
    "if len(x) > 0:\n",
    "    raise ValueError(f\"Duplicate values found :\\n{duplicate_counts}\")\n",
    "else:\n",
    "# # Proceed with the rest of the code if no duplicates\n",
    "    print(\"No duplicates found. Continuing execution...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a54e850f",
   "metadata": {
    "tags": [
     "papermill-error-cell-tag"
    ]
   },
   "source": [
    "<span id=\"papermill-error-cell\" style=\"color:red; font-family:Helvetica Neue, Helvetica, Arial, sans-serif; font-size:2em;\">Execution using papermill encountered an exception here and stopped:</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "72a2cc41",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-28T11:05:12.324603Z",
     "iopub.status.busy": "2024-12-28T11:05:12.320726Z",
     "iopub.status.idle": "2024-12-28T11:05:34.519064Z",
     "shell.execute_reply": "2024-12-28T11:05:34.507278Z"
    },
    "papermill": {
     "duration": 22.244104,
     "end_time": "2024-12-28T11:05:34.553258",
     "exception": true,
     "start_time": "2024-12-28T11:05:12.309154",
     "status": "failed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "[DELTA_UNSUPPORTED_SUBQUERY] Subqueries are not supported in the DELETE (condition = ((NOT (target.PKSK IN (listquery()))) AND (target.DateSK IN (listquery())))).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 20\u001b[0m\n\u001b[1;32m     12\u001b[0m     spark\u001b[38;5;241m.\u001b[39msql(query)        \n\u001b[1;32m     13\u001b[0m     query \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;124m            DELETE FROM delta.`\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrgt_path_processed\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` AS target\u001b[39m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;124m            WHERE target.PKSK NOT IN (SELECT PKSK FROM vw_source)\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;124m            )\u001b[39m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;124m            \u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m---> 20\u001b[0m     \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m :\n\u001b[1;32m     22\u001b[0m     query\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCREATE TABLE delta.`\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrgt_path_processed\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` USING DELTA AS SELECT * FROM vw_source\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pyspark/sql/session.py:1631\u001b[0m, in \u001b[0;36mSparkSession.sql\u001b[0;34m(self, sqlQuery, args, **kwargs)\u001b[0m\n\u001b[1;32m   1627\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1628\u001b[0m         litArgs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mPythonUtils\u001b[38;5;241m.\u001b[39mtoArray(\n\u001b[1;32m   1629\u001b[0m             [_to_java_column(lit(v)) \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m (args \u001b[38;5;129;01mor\u001b[39;00m [])]\n\u001b[1;32m   1630\u001b[0m         )\n\u001b[0;32m-> 1631\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[43msqlQuery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlitArgs\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m   1632\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   1633\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(kwargs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: [DELTA_UNSUPPORTED_SUBQUERY] Subqueries are not supported in the DELETE (condition = ((NOT (target.PKSK IN (listquery()))) AND (target.DateSK IN (listquery()))))."
     ]
    }
   ],
   "source": [
    "if DeltaTable.isDeltaTable(spark, trgt_path_processed):\n",
    "    column_name = df_output.columns\n",
    "    set_clause = \", \".join([f\"target.{i} = source.{i}\" for i in column_name])\n",
    "    insert_clause=\",\".join(column_name)\n",
    "    insert_values=\",\".join([f\"source.{i}\" for i in column_name])\n",
    "    query = f\"\"\"MERGE INTO delta.`{trgt_path_processed}` AS target \n",
    "            USING vw_source AS source \n",
    "            ON target.PKSK = source.PKSK \n",
    "            AND target.RowSK <> source.RowSK \n",
    "            WHEN MATCHED THEN UPDATE SET {set_clause}\n",
    "            WHEN NOT MATCHED THEN INSERT ({insert_clause}) VALUES ({insert_values})\"\"\"\n",
    "    spark.sql(query)        \n",
    "    query = f\"\"\"\n",
    "            DELETE FROM delta.`{trgt_path_processed}` AS target\n",
    "            WHERE target.PKSK NOT IN (SELECT PKSK FROM vw_source)\n",
    "            AND target.DateSK IN (\n",
    "                SELECT DateSK FROM vw_source\n",
    "            )\n",
    "            \"\"\"\n",
    "    spark.sql(query)\n",
    "else :\n",
    "    query=f\"CREATE TABLE delta.`{trgt_path_processed}` USING DELTA AS SELECT * FROM vw_source\"\n",
    "    spark.sql(query)\n",
    "\n",
    "print(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b92b760",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Save the DataFrame to a CSV file\n",
    "spark.read.format(\"delta\").load(trgt_path_processed).coalesce(1).write.format(\"csv\").option(\"header\",\"true\").mode(\"overwrite\").save(trgt_path_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02aa05c5",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "spark.read.format(\"csv\").option(\"header\",\"true\").load(trgt_path_csv).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34665d65",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "spark.sql(f\"\"\"SELECT UpdateTimeStamp FROM delta.`{trgt_path_processed}` \n",
    "          WHERE UpdateTimeStamp = (SELECT MAX(UpdateTimeStamp) FROM vw_source)\"\"\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33e2878b",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "trgt_copy_path = trgt_path_csv + \"processed.csv\"\n",
    "files=os.listdir(trgt_path_csv)\n",
    "selected_files = [file for file in files if file.startswith('part-00') and file.endswith('.csv')]\n",
    "file=trgt_path_csv + selected_files[0]\n",
    "print(selected_files)\n",
    "shutil.copy(file, trgt_copy_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e695131e",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "delete_log = [file for file in files if \"processed.csv\" != file ]\n",
    "for file in delete_log :\n",
    "    os.remove(trgt_path_csv + file)\n",
    "    print(f\"removed {trgt_path_csv + file}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 111.393488,
   "end_time": "2024-12-28T11:05:37.247746",
   "environment_variables": {},
   "exception": true,
   "input_path": "/home/jovyan/Notebooks/NB_FACT_NTBK.ipynb",
   "output_path": "/home/jovyan/Notebooks/output/NB_FACT_NTBK.ipynb",
   "parameters": {},
   "start_time": "2024-12-28T11:03:45.854258",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
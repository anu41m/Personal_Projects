{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.utils import *\n",
    "from delta import *\n",
    "from datetime import *\n",
    "import hashlib\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)  # Show all columns\n",
    "pd.set_option('display.width', 1000)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:KeyboardInterrupt while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/socket.py\", line 706, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m builder \u001b[38;5;241m=\u001b[39m pyspark\u001b[38;5;241m.\u001b[39msql\u001b[38;5;241m.\u001b[39mSparkSession\u001b[38;5;241m.\u001b[39mbuilder\u001b[38;5;241m.\u001b[39mappName(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNB_Fact\u001b[39m\u001b[38;5;124m\"\u001b[39m) \n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Spark will automatically use the master specified in spark-defaults.conf\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m spark \u001b[38;5;241m=\u001b[39m \u001b[43mconfigure_spark_with_delta_pip\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbuilder\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetOrCreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m spark\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/session.py:497\u001b[0m, in \u001b[0;36mSparkSession.Builder.getOrCreate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    495\u001b[0m     sparkConf\u001b[38;5;241m.\u001b[39mset(key, value)\n\u001b[1;32m    496\u001b[0m \u001b[38;5;66;03m# This SparkContext may be an existing one.\u001b[39;00m\n\u001b[0;32m--> 497\u001b[0m sc \u001b[38;5;241m=\u001b[39m \u001b[43mSparkContext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetOrCreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msparkConf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    498\u001b[0m \u001b[38;5;66;03m# Do not update `SparkConf` for existing `SparkContext`, as it's shared\u001b[39;00m\n\u001b[1;32m    499\u001b[0m \u001b[38;5;66;03m# by all sessions.\u001b[39;00m\n\u001b[1;32m    500\u001b[0m session \u001b[38;5;241m=\u001b[39m SparkSession(sc, options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_options)\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/context.py:515\u001b[0m, in \u001b[0;36mSparkContext.getOrCreate\u001b[0;34m(cls, conf)\u001b[0m\n\u001b[1;32m    513\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m    514\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 515\u001b[0m         \u001b[43mSparkContext\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconf\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mSparkConf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    516\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    517\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/context.py:203\u001b[0m, in \u001b[0;36mSparkContext.__init__\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls, udf_profiler_cls, memory_profiler_cls)\u001b[0m\n\u001b[1;32m    201\u001b[0m SparkContext\u001b[38;5;241m.\u001b[39m_ensure_initialized(\u001b[38;5;28mself\u001b[39m, gateway\u001b[38;5;241m=\u001b[39mgateway, conf\u001b[38;5;241m=\u001b[39mconf)\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 203\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_init\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    204\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaster\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    205\u001b[0m \u001b[43m        \u001b[49m\u001b[43mappName\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[43m        \u001b[49m\u001b[43msparkHome\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    207\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpyFiles\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    208\u001b[0m \u001b[43m        \u001b[49m\u001b[43menvironment\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    209\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatchSize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    210\u001b[0m \u001b[43m        \u001b[49m\u001b[43mserializer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    211\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    212\u001b[0m \u001b[43m        \u001b[49m\u001b[43mjsc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    213\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprofiler_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    214\u001b[0m \u001b[43m        \u001b[49m\u001b[43mudf_profiler_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    215\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmemory_profiler_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    216\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m:\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;66;03m# If an error occurs, clean up in order to allow future SparkContext creation:\u001b[39;00m\n\u001b[1;32m    219\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop()\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/context.py:296\u001b[0m, in \u001b[0;36mSparkContext._do_init\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, jsc, profiler_cls, udf_profiler_cls, memory_profiler_cls)\u001b[0m\n\u001b[1;32m    293\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menvironment[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPYTHONHASHSEED\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39menviron\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPYTHONHASHSEED\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    295\u001b[0m \u001b[38;5;66;03m# Create the Java SparkContext through Py4J\u001b[39;00m\n\u001b[0;32m--> 296\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jsc \u001b[38;5;241m=\u001b[39m jsc \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_initialize_context\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jconf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[38;5;66;03m# Reset the SparkConf to the one actually used by the SparkContext in JVM.\u001b[39;00m\n\u001b[1;32m    298\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_conf \u001b[38;5;241m=\u001b[39m SparkConf(_jconf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jsc\u001b[38;5;241m.\u001b[39msc()\u001b[38;5;241m.\u001b[39mconf())\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/context.py:421\u001b[0m, in \u001b[0;36mSparkContext._initialize_context\u001b[0;34m(self, jconf)\u001b[0m\n\u001b[1;32m    417\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    418\u001b[0m \u001b[38;5;124;03mInitialize SparkContext in function to allow subclass specific initialization\u001b[39;00m\n\u001b[1;32m    419\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    420\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 421\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mJavaSparkContext\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjconf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1586\u001b[0m, in \u001b[0;36mJavaClass.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1578\u001b[0m args_command \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\n\u001b[1;32m   1579\u001b[0m     [get_command_part(arg, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pool) \u001b[38;5;28;01mfor\u001b[39;00m arg \u001b[38;5;129;01min\u001b[39;00m new_args])\n\u001b[1;32m   1581\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCONSTRUCTOR_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1582\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_command_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1583\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1584\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m-> 1586\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gateway_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1587\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[1;32m   1588\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gateway_client, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fqn)\n\u001b[1;32m   1590\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1038\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1036\u001b[0m connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_connection()\n\u001b[1;32m   1037\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1038\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1039\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m binary:\n\u001b[1;32m   1040\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m response, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_connection_guard(connection)\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:511\u001b[0m, in \u001b[0;36mClientServerConnection.send_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m    509\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    510\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 511\u001b[0m         answer \u001b[38;5;241m=\u001b[39m smart_decode(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream\u001b[38;5;241m.\u001b[39mreadline()[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m    512\u001b[0m         logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnswer received: \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(answer))\n\u001b[1;32m    513\u001b[0m         \u001b[38;5;66;03m# Happens when a the other end is dead. There might be an empty\u001b[39;00m\n\u001b[1;32m    514\u001b[0m         \u001b[38;5;66;03m# answer before the socket raises an error.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/socket.py:706\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    704\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 706\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    707\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    708\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from delta import *\n",
    "\n",
    "builder = pyspark.sql.SparkSession.builder.appName(\"NB_Fact\") \n",
    "# Spark will automatically use the master specified in spark-defaults.conf\n",
    "spark = configure_spark_with_delta_pip(builder).getOrCreate()\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "trgt_path_processed = \"/mnt/Fact/Fact_Parquet\"\n",
    "trgt_path_csv = \"/mnt/Fact/Fact_Processed\"\n",
    "mount_d =\"/data/\"\n",
    "trgt_path = '/mnt/'\n",
    "source_path = mount_d+\"Template.xlsm\"\n",
    "sheet_name ='SPENDING_HISTORY'\n",
    "calendar_path =\"/mnt/Calendar/Calendar_Parquet/\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'datetime.date'>\n",
      "2000-01-01\n"
     ]
    }
   ],
   "source": [
    "if DeltaTable.isDeltaTable(spark, trgt_path_processed):\n",
    "    max_date = spark.sql(f\"select max(date) from delta.`{calendar_path}` WHERE datesk = (select max(datesk) from delta.`{trgt_path_processed}`)\").collect()[0][0] \n",
    "else:\n",
    "    max_date= spark.sql(f\"select min(date) from delta.`{calendar_path}`\").collect()[0][0]\n",
    "max_date= spark.sql(f\"select min(date) from delta.`{calendar_path}`\").collect()[0][0]\n",
    "print(type(max_date))\n",
    "print(max_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "pandas_df = pd.read_excel(source_path, sheet_name = sheet_name)\n",
    "df=spark.createDataFrame(pandas_df)\n",
    "df= df.withColumn(\"Date\", date_format(df[\"Date\"], \"yyyy-MM-dd\").cast(\"date\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+--------------+--------------------+---------------+------------+\n",
      "|Index|      Date| Spending Item|         Wallet used|       Category|Spend Amount|\n",
      "+-----+----------+--------------+--------------------+---------------+------------+\n",
      "|    1|2024-10-01|      Bus Fare|           ICICI Pay|     Travelling|         271|\n",
      "|    2|2024-10-01|   Zomato Food|       Amazon Wallet|           Food|         436|\n",
      "|    3|2024-10-01|Phone Recharge|           ICICI Pay|Mobile Recharge|         359|\n",
      "|    4|2024-10-01|Phone Recharge|Credit Card - Amazon|Mobile Recharge|         118|\n",
      "|    5|2024-10-01|  food from FC|               G-pay|           Food|         120|\n",
      "|    6|2024-10-01|   Zomato Food|       Amazon Wallet|           Food|         160|\n",
      "|    7|2024-10-01|     Groceries|           ICICI Pay|           Food|         245|\n",
      "|    8|2024-10-02|         Iwish|           ICICI Pay|     Investment|         745|\n",
      "|    9|2024-10-02|      TVM Rent|               G-pay|           Rent|        3250|\n",
      "|   10|2024-10-02| Gold Invstmnt|Credit Card - Amazon|     Investment|        1000|\n",
      "|   11|2024-10-03|   Zomato Food|       Amazon Wallet|           Food|         170|\n",
      "|   12|2024-10-04|   Zomato Food|       Amazon Wallet|           Food|         154|\n",
      "|   13|2024-10-05|Phone Recharge|Credit Card - Amazon|Mobile Recharge|         359|\n",
      "|   14|2024-10-06|     Food Mess|               G-pay|           Food|         760|\n",
      "|   15|2024-10-07|  food from FC|               G-pay|           Food|         125|\n",
      "|   16|2024-10-07|     Groceries|               G-pay|           Food|         257|\n",
      "|   17|2024-10-07|   KSFE Chitti|           ICICI Pay|         Chitty|       13194|\n",
      "|   18|2024-10-09|    Investment|           ICICI Pay|     Investment|       15000|\n",
      "|   19|2024-10-09|    Investment|           ICICI Pay|     Investment|        2500|\n",
      "|   20|2024-10-09|    Investment|           ICICI Pay|     Investment|        2780|\n",
      "+-----+----------+--------------+--------------------+---------------+------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "43\n"
     ]
    }
   ],
   "source": [
    "df.createOrReplaceTempView(\"vw_src\")\n",
    "df_src = spark.sql(f\"select * from vw_src where Date >= '{max_date}'\")\n",
    "df_src.show()\n",
    "print(df_src.count())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temp =(\n",
    "df_src\n",
    "    .withColumnRenamed(\"Spending Item\", \"SpendingItem\")\n",
    "    .withColumnRenamed(\"Spend Amount\", \"SpendAmount\")\n",
    "    .withColumnRenamed(\"Wallet used\", \"WalletUsed\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_output =(\n",
    "    df_temp\n",
    "    .withColumn(\"WalletSK\", xxhash64(\"WalletUsed\"))\n",
    "    .withColumn(\"categorysk\", xxhash64(\"category\").cast(\"long\"))\n",
    "    .withColumn(\"DateSK\", regexp_replace(\"date\", \"-\", \"\").cast(\"string\"))\n",
    "    .withColumn(\"PKSK\", xxhash64(concat(\"category\", \"WalletUsed\", \"Index\", \"Date\")).cast(\"string\"))\n",
    "    .withColumn(\"UpdateTimeStamp\", date_format(current_timestamp(), \"yyyy-MM-dd HH:mm:ss\").cast(\"timestamp\"))\n",
    "    .withColumn(\"RowSK\", xxhash64(concat_ws(\"|\", *[col(c) for c in df_temp.columns])))\n",
    "    .drop(\"Index\", \"Date\", \"category\",\"WalletUsed\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----------+--------------------+--------------------+--------+--------------------+-------------------+--------------------+\n",
      "|  SpendingItem|SpendAmount|            WalletSK|          categorysk|  DateSK|                PKSK|    UpdateTimeStamp|               RowSK|\n",
      "+--------------+-----------+--------------------+--------------------+--------+--------------------+-------------------+--------------------+\n",
      "|      Bus Fare|        271| 6156857810389859050|-6960913322501014326|20241001|-6565386604912707528|2024-11-21 16:04:44| 6693242557264883629|\n",
      "|   Zomato Food|        436|-2996829708924805941|-1486987501743003025|20241001| 5938848620272978843|2024-11-21 16:04:44| 4075940645094743113|\n",
      "|Phone Recharge|        359| 6156857810389859050|-6072606072650303353|20241001| 4210959322690640852|2024-11-21 16:04:44|-8240864996273807886|\n",
      "|Phone Recharge|        118| 4962050552325147656|-6072606072650303353|20241001|  813826478454288165|2024-11-21 16:04:44|-7125262503767821305|\n",
      "|  food from FC|        120| 1782127853235431588|-1486987501743003025|20241001| 5134688205544070153|2024-11-21 16:04:44|-7219489668109678195|\n",
      "|   Zomato Food|        160|-2996829708924805941|-1486987501743003025|20241001|-4554691111434300945|2024-11-21 16:04:44| 9128687197103399914|\n",
      "|     Groceries|        245| 6156857810389859050|-1486987501743003025|20241001| 1488336239849999432|2024-11-21 16:04:44|-9210681773154980992|\n",
      "|         Iwish|        745| 6156857810389859050| 1987224080996593371|20241002| 6720896348394801887|2024-11-21 16:04:44| 6611452448473101605|\n",
      "|      TVM Rent|       3250| 1782127853235431588|  858271923021320980|20241002| -791442438948482998|2024-11-21 16:04:44| 5132791079739298696|\n",
      "| Gold Invstmnt|       1000| 4962050552325147656| 1987224080996593371|20241002|  421107018635261412|2024-11-21 16:04:44| 2519123205007771843|\n",
      "|   Zomato Food|        170|-2996829708924805941|-1486987501743003025|20241003|-2373941341638821577|2024-11-21 16:04:44|  871000720615682750|\n",
      "|   Zomato Food|        154|-2996829708924805941|-1486987501743003025|20241004|-1479665226908810344|2024-11-21 16:04:44|-4815796986919341310|\n",
      "|Phone Recharge|        359| 4962050552325147656|-6072606072650303353|20241005|-3407533654855836773|2024-11-21 16:04:44|-4632363966665284175|\n",
      "|     Food Mess|        760| 1782127853235431588|-1486987501743003025|20241006|-2523993665234743902|2024-11-21 16:04:44| 4131036530357465085|\n",
      "|  food from FC|        125| 1782127853235431588|-1486987501743003025|20241007| 6796290013836075436|2024-11-21 16:04:44| 7142617218034287737|\n",
      "|     Groceries|        257| 1782127853235431588|-1486987501743003025|20241007|-4409587861837255688|2024-11-21 16:04:44|-3219307325814528153|\n",
      "|   KSFE Chitti|      13194| 6156857810389859050|-4245010722904360951|20241007|-2476664503748536262|2024-11-21 16:04:44| 8478567904763550513|\n",
      "|    Investment|      15000| 6156857810389859050| 1987224080996593371|20241009|-9059424830132342410|2024-11-21 16:04:44|-6650782551861317464|\n",
      "|    Investment|       2500| 6156857810389859050| 1987224080996593371|20241009|    2722928406332133|2024-11-21 16:04:44| 8614935000444536746|\n",
      "|    Investment|       2780| 6156857810389859050| 1987224080996593371|20241009| 3683721171785163099|2024-11-21 16:04:44|-1746789695994716117|\n",
      "+--------------+-----------+--------------------+--------------------+--------+--------------------+-------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_output.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No duplicates found. Continuing execution...\n"
     ]
    }
   ],
   "source": [
    "df_output.createOrReplaceTempView(\"vw_source\")\n",
    "\n",
    "duplicate_counts = spark.sql(\"\"\"\n",
    "    SELECT COUNT(PKSK) as count\n",
    "    FROM vw_source\n",
    "    GROUP BY PKSK\n",
    "    HAVING COUNT(PKSK) > 1\n",
    "\"\"\")\n",
    "x = [row['count'] for row in duplicate_counts.collect()]\n",
    "\n",
    "# Fail the code if there are duplicates\n",
    "if len(x) > 0:\n",
    "    raise ValueError(f\"Duplicate values found :\\n{duplicate_counts}\")\n",
    "else:\n",
    "# # Proceed with the rest of the code if no duplicates\n",
    "    print(\"No duplicates found. Continuing execution...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MERGE INTO delta.`/mnt/Fact/Fact_Parquet` AS target \n",
      "            USING vw_source AS source \n",
      "            ON target.PKSK = source.PKSK \n",
      "            AND target.RowSK <> source.RowSK \n",
      "            WHEN MATCHED THEN UPDATE SET target.SpendingItem = source.SpendingItem, target.SpendAmount = source.SpendAmount, target.WalletSK = source.WalletSK, target.categorysk = source.categorysk, target.DateSK = source.DateSK, target.PKSK = source.PKSK, target.UpdateTimeStamp = source.UpdateTimeStamp, target.RowSK = source.RowSK\n"
     ]
    }
   ],
   "source": [
    "column_name = df_output.columns\n",
    "set_clause = \", \".join([f\"target.{i} = source.{i}\" for i in column_name])\n",
    "query = f\"\"\"MERGE INTO delta.`{trgt_path_processed}` AS target \n",
    "            USING vw_source AS source \n",
    "            ON target.PKSK = source.PKSK \n",
    "            AND target.RowSK <> source.RowSK \n",
    "            WHEN MATCHED THEN UPDATE SET {set_clause}\"\"\"\n",
    "print(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DeltaTable.isDeltaTable(spark, trgt_path_processed):\n",
    "    spark.sql(query)\n",
    "else :\n",
    "    spark.sql(f\"CREATE TABLE delta.`{trgt_path_processed}` USING DELTA AS SELECT * FROM vw_source\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the DataFrame to a CSV file\n",
    "spark.read.format(\"delta\").load(trgt_path_processed).coalesce(1).write.format(\"csv\").option(\"header\",\"true\").mode(\"overwrite\").save(trgt_path_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.read.format(\"csv\").option(\"header\",\"true\").load(trgt_path_csv).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(f\"\"\"SELECT UpdateTimeStamp FROM delta.`{trgt_path_processed}` \n",
    "          WHERE UpdateTimeStamp = (SELECT MAX(UpdateTimeStamp) FROM vw_source)\"\"\").count()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
